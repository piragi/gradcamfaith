{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9614cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING VISUALIZATION DEBUG\n",
      "==================================================\n",
      "✓ Basic matplotlib works\n",
      "=== DEBUGGING DATA STRUCTURE ===\n",
      "Similarities file exists: True\n",
      "Importance file exists: True\n",
      "\n",
      "--- Loading similarities data ---\n",
      "Number of images: 6409\n",
      "First image key: bcc_ISIC_0031789\n",
      "Keys in first image data: dict_keys(['predicted_class', 'similarities'])\n",
      "Predicted class: 5\n",
      "Similarity keys: [0, 1, 2, 3, 4, 6, 'all']\n",
      "'all' similarities shape: (5, 12, 197)\n",
      "Expected: (layers, heads, tokens)\n",
      "Classes found in data: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "--- Loading importance data ---\n",
      "Head importance keys: ['class_head_importance', 'class_ranked_heads', 'images_by_class', 'class_image_counts', 'similarity_stats', 'top_heads_per_class']\n",
      "Classes in importance data: [0, 1, 2, 3, 4, 5, 6]\n",
      "Class 0 importance shape: (5, 12)\n",
      "Class 1 importance shape: (5, 12)\n",
      "Class 2 importance shape: (5, 12)\n",
      "Class 3 importance shape: (5, 12)\n",
      "Class 4 importance shape: (5, 12)\n",
      "Class 5 importance shape: (5, 12)\n",
      "Class 6 importance shape: (5, 12)\n",
      "Image counts per class: {0: 211, 1: 276, 2: 511, 3: 64, 4: 1735, 5: 3508, 6: 104}\n",
      "\n",
      "--- Checking token patterns ---\n",
      "Token patterns for class 0: ✓\n",
      "  Keys: ['n_images', 'clusters', 'explained_variance', 'cluster_labels', 'pca_result']\n",
      "  Images: 211\n",
      "Token patterns for class 1: ✓\n",
      "  Keys: ['n_images', 'clusters', 'explained_variance', 'cluster_labels', 'pca_result']\n",
      "  Images: 276\n",
      "Token patterns for class 2: ✓\n",
      "  Keys: ['n_images', 'clusters', 'explained_variance', 'cluster_labels', 'pca_result']\n",
      "  Images: 511\n",
      "Token patterns for class 3: ✓\n",
      "  Keys: ['n_images', 'clusters', 'explained_variance', 'cluster_labels', 'pca_result']\n",
      "  Images: 64\n",
      "Token patterns for class 4: ✓\n",
      "  Keys: ['n_images', 'clusters', 'explained_variance', 'cluster_labels', 'pca_result']\n",
      "  Images: 1735\n",
      "Token patterns for class 5: ✓\n",
      "  Keys: ['n_images', 'clusters', 'explained_variance', 'cluster_labels', 'pca_result']\n",
      "  Images: 3508\n",
      "Token patterns for class 6: ✓\n",
      "  Keys: ['n_images', 'clusters', 'explained_variance', 'cluster_labels', 'pca_result']\n",
      "  Images: 104\n",
      "\n",
      "=== ATTEMPTING SAFE VISUALIZATION ===\n",
      "\n",
      "=== TESTING HEAD IMPORTANCE VISUALIZATION ===\n",
      "Available classes: [0, 1, 2, 3, 4, 5, 6]\n",
      "Data shape per class: (5, 12) (layers x heads)\n",
      "✓ Head importance visualization saved to results/train/head_analysis/debug_visualizations/debug_head_importance.png\n",
      "\n",
      "✅ DEBUG VISUALIZATION SUCCESSFUL!\n",
      "Check results in: results/train/head_analysis/debug_visualizations\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Debug version of HAM10000 Head Analysis Visualization Module\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "def debug_data_structure(data_dir: Path):\n",
    "    \"\"\"Debug function to inspect the data structure before visualization.\"\"\"\n",
    "    \n",
    "    print(\"=== DEBUGGING DATA STRUCTURE ===\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    similarities_path = data_dir / \"head_direction_similarities.npy\"\n",
    "    importance_path = data_dir / \"head_importance_analysis.npy\"\n",
    "    \n",
    "    print(f\"Similarities file exists: {similarities_path.exists()}\")\n",
    "    print(f\"Importance file exists: {importance_path.exists()}\")\n",
    "    \n",
    "    if not similarities_path.exists() or not importance_path.exists():\n",
    "        print(\"❌ Required data files not found!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Load and inspect similarities\n",
    "        print(\"\\n--- Loading similarities data ---\")\n",
    "        direction_similarities = np.load(similarities_path, allow_pickle=True).item()\n",
    "        print(f\"Number of images: {len(direction_similarities)}\")\n",
    "        \n",
    "        # Check first image to understand structure\n",
    "        first_key = list(direction_similarities.keys())[0]\n",
    "        first_data = direction_similarities[first_key]\n",
    "        print(f\"First image key: {first_key}\")\n",
    "        print(f\"Keys in first image data: {first_data.keys()}\")\n",
    "        print(f\"Predicted class: {first_data['predicted_class']}\")\n",
    "        \n",
    "        # Check similarities structure\n",
    "        similarities = first_data['similarities']\n",
    "        print(f\"Similarity keys: {list(similarities.keys())}\")\n",
    "        \n",
    "        if 'all' in similarities:\n",
    "            sim_shape = similarities['all'].shape\n",
    "            print(f\"'all' similarities shape: {sim_shape}\")\n",
    "            print(f\"Expected: (layers, heads, tokens)\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        classes_found = set()\n",
    "        for img_data in direction_similarities.values():\n",
    "            classes_found.add(img_data['predicted_class'])\n",
    "        print(f\"Classes found in data: {sorted(classes_found)}\")\n",
    "        \n",
    "        # Load and inspect importance\n",
    "        print(\"\\n--- Loading importance data ---\")\n",
    "        head_importance = np.load(importance_path, allow_pickle=True).item()\n",
    "        print(f\"Head importance keys: {list(head_importance.keys())}\")\n",
    "        \n",
    "        if 'class_head_importance' in head_importance:\n",
    "            class_imp = head_importance['class_head_importance']\n",
    "            print(f\"Classes in importance data: {list(class_imp.keys())}\")\n",
    "            \n",
    "            # Check shapes for each class\n",
    "            for cls_idx, imp_data in class_imp.items():\n",
    "                print(f\"Class {cls_idx} importance shape: {imp_data.shape}\")\n",
    "        \n",
    "        if 'class_image_counts' in head_importance:\n",
    "            counts = head_importance['class_image_counts']\n",
    "            print(f\"Image counts per class: {counts}\")\n",
    "        \n",
    "        # Check token patterns\n",
    "        print(\"\\n--- Checking token patterns ---\")\n",
    "        token_patterns = {}\n",
    "        for class_idx in range(7):\n",
    "            pattern_path = data_dir / f\"token_patterns_class_{class_idx}.npy\"\n",
    "            if pattern_path.exists():\n",
    "                try:\n",
    "                    patterns = np.load(pattern_path, allow_pickle=True).item()\n",
    "                    token_patterns[class_idx] = patterns\n",
    "                    print(f\"Token patterns for class {class_idx}: ✓\")\n",
    "                    print(f\"  Keys: {list(patterns.keys())}\")\n",
    "                    if 'n_images' in patterns:\n",
    "                        print(f\"  Images: {patterns['n_images']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Token patterns for class {class_idx}: ❌ ({e})\")\n",
    "            else:\n",
    "                print(f\"Token patterns for class {class_idx}: File not found\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def safe_visualize_head_importance(head_importance: Dict, save_dir: Path):\n",
    "    \"\"\"Safer version of head importance visualization with error handling.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== TESTING HEAD IMPORTANCE VISUALIZATION ===\")\n",
    "    \n",
    "    try:\n",
    "        importance_data = head_importance['class_head_importance']\n",
    "        available_classes = list(importance_data.keys())\n",
    "        print(f\"Available classes: {available_classes}\")\n",
    "        \n",
    "        # Determine actual number of classes and shape\n",
    "        first_class = available_classes[0]\n",
    "        shape = importance_data[first_class].shape\n",
    "        num_layers, num_heads = shape\n",
    "        print(f\"Data shape per class: {shape} (layers x heads)\")\n",
    "        \n",
    "        # Create figure based on actual number of classes\n",
    "        n_classes = len(available_classes)\n",
    "        if n_classes <= 3:\n",
    "            fig, axes = plt.subplots(1, n_classes, figsize=(6*n_classes, 6))\n",
    "            if n_classes == 1:\n",
    "                axes = [axes]\n",
    "        elif n_classes <= 4:\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "        else:\n",
    "            # For more than 4 classes, use 2 rows\n",
    "            n_cols = min(4, n_classes)\n",
    "            n_rows = (n_classes + n_cols - 1) // n_cols\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6*n_rows))\n",
    "            axes = axes.flatten() if n_classes > 1 else [axes]\n",
    "        \n",
    "        class_names = {\n",
    "            0: 'akiec', 1: 'bcc', 2: 'bkl', 3: 'df', 4: 'mel', 5: 'nv', 6: 'vasc'\n",
    "        }\n",
    "        \n",
    "        class_colors = {\n",
    "            0: '#FF6B6B', 1: '#4ECDC4', 2: '#45B7D1', 3: '#96CEB4',\n",
    "            4: '#FFEAA7', 5: '#DDA0DD', 6: '#FFA07A'\n",
    "        }\n",
    "        \n",
    "        for i, class_idx in enumerate(available_classes):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            ax = axes[i]\n",
    "            heatmap_data = importance_data[class_idx]\n",
    "            \n",
    "            # Create colormap\n",
    "            colors = ['white', class_colors.get(class_idx, '#888888')]\n",
    "            cmap = sns.blend_palette(colors, as_cmap=True)\n",
    "            \n",
    "            # Plot heatmap\n",
    "            im = ax.imshow(heatmap_data, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
    "            \n",
    "            # Customize\n",
    "            ax.set_xlabel('Head Index', fontsize=12)\n",
    "            ax.set_ylabel('Layer', fontsize=12)\n",
    "            class_name = class_names.get(class_idx, f'Class_{class_idx}')\n",
    "            ax.set_title(f'{class_name.upper()} Head Importance', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Set ticks\n",
    "            ax.set_xticks(range(num_heads))\n",
    "            ax.set_yticks(range(num_layers))\n",
    "            ax.set_yticklabels([f'L{7 + i}' for i in range(num_layers)])  # Assuming last 5 layers\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label('Importance Score', fontsize=10)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(available_classes), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Head Importance Analysis (Debug Version)', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        save_path = save_dir / 'debug_head_importance.png'\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✓ Head importance visualization saved to {save_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in head importance visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_basic_plot():\n",
    "    \"\"\"Test if basic matplotlib works.\"\"\"\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.plot([1, 2, 3], [1, 4, 2])\n",
    "        ax.set_title(\"Basic Test Plot\")\n",
    "        plt.close()\n",
    "        print(\"✓ Basic matplotlib works\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Basic matplotlib test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def debug_and_test_visualization(data_dir: Path):\n",
    "    \"\"\"Main debug function.\"\"\"\n",
    "    \n",
    "    print(\"STARTING VISUALIZATION DEBUG\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test 1: Basic matplotlib\n",
    "    if not test_basic_plot():\n",
    "        return\n",
    "    \n",
    "    # Test 2: Data structure\n",
    "    if not debug_data_structure(data_dir):\n",
    "        return\n",
    "    \n",
    "    # Test 3: Try basic visualization\n",
    "    try:\n",
    "        print(\"\\n=== ATTEMPTING SAFE VISUALIZATION ===\")\n",
    "        \n",
    "        # Load data\n",
    "        importance_path = data_dir / \"head_importance_analysis.npy\"\n",
    "        head_importance = np.load(importance_path, allow_pickle=True).item()\n",
    "        \n",
    "        # Create viz directory\n",
    "        viz_dir = data_dir / \"debug_visualizations\"\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "        \n",
    "        # Try safe head importance visualization\n",
    "        success = safe_visualize_head_importance(head_importance, viz_dir)\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\n✅ DEBUG VISUALIZATION SUCCESSFUL!\")\n",
    "            print(f\"Check results in: {viz_dir}\")\n",
    "        else:\n",
    "            print(\"\\n❌ Visualization failed even in safe mode\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Critical error in debug visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = Path(\"./results/train/head_analysis\")  # Update this path\n",
    "    debug_and_test_visualization(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da01ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HAM10000 analysis data...\n",
      "Generating comprehensive visualizations for HAM10000 analysis...\n",
      "1/8 - Head Importance Heatmaps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piragi/projects/gradcamfaithkvasir/gradcamfaith/head_analysis_visualization.py:185: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/8 - Similarity Distributions...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      4\u001b[0m data_directory \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/train/head_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mhead_analysis_visualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_comprehensive_visualizations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/gradcamfaithkvasir/gradcamfaith/head_analysis_visualization.py:1065\u001b[0m, in \u001b[0;36mcreate_comprehensive_visualizations\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     if pattern_path.exists():\n\u001b[1;32m   1063\u001b[0m         token_patterns[class_idx] = np.load(pattern_path, allow_pickle=True).item()\n\u001b[0;32m-> 1065\u001b[0m # Create visualizer and generate all visualizations\n\u001b[1;32m   1066\u001b[0m visualizer = HeadAnalysisVisualizer(data_dir)\n\u001b[1;32m   1067\u001b[0m visualizer.generate_all_visualizations(\n\u001b[1;32m   1068\u001b[0m     direction_similarities, head_importance, token_patterns, num_layers=5, start_layer=7\n\u001b[1;32m   1069\u001b[0m )\n",
      "File \u001b[0;32m~/projects/gradcamfaithkvasir/gradcamfaith/head_analysis_visualization.py:88\u001b[0m, in \u001b[0;36mHeadAnalysisVisualizer.generate_all_visualizations\u001b[0;34m(self, direction_similarities, head_importance, token_patterns, num_layers, start_layer)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# 2. Similarity Distribution Analysis\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2/8 - Similarity Distributions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_visualize_similarity_distributions(direction_similarities)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_memory()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 3. Top Contributing Heads Comparison\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/gradcamfaithkvasir/gradcamfaith/head_analysis_visualization.py:222\u001b[0m, in \u001b[0;36m_visualize_similarity_distributions\u001b[0;34m(self, direction_similarities)\u001b[0m\n\u001b[1;32m    219\u001b[0m     class_similarities[class_idx].extend(similarities)\n\u001b[1;32m    220\u001b[0m     all_similarities.extend(similarities)\n\u001b[0;32m--> 222\u001b[0m # Plot 1: Overall distribution\n\u001b[1;32m    223\u001b[0m ax = axes[0]\n\u001b[1;32m    224\u001b[0m ax.hist(all_similarities, bins=50, alpha=0.7, color='gray', edgecolor='black')\n",
      "File \u001b[0;32m~/projects/gradcamfaithkvasir/gradcamfaith/.venv/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:453\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    448\u001b[0m     warn_deprecated(\n\u001b[1;32m    449\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    452\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/gradcamfaithkvasir/gradcamfaith/.venv/lib/python3.10/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/projects/gradcamfaithkvasir/gradcamfaith/.venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:7042\u001b[0m, in \u001b[0;36mAxes.hist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   7039\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   7041\u001b[0m \u001b[38;5;66;03m# Massage 'x' for processing.\u001b[39;00m\n\u001b[0;32m-> 7042\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reshape_2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7043\u001b[0m nx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)  \u001b[38;5;66;03m# number of datasets\u001b[39;00m\n\u001b[1;32m   7045\u001b[0m \u001b[38;5;66;03m# Process unit information.  _process_unit_info sets the unit and\u001b[39;00m\n\u001b[1;32m   7046\u001b[0m \u001b[38;5;66;03m# converts the first dataset; then we convert each following dataset\u001b[39;00m\n\u001b[1;32m   7047\u001b[0m \u001b[38;5;66;03m# one at a time.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/gradcamfaithkvasir/gradcamfaith/.venv/lib/python3.10/site-packages/matplotlib/cbook.py:1412\u001b[0m, in \u001b[0;36m_reshape_2D\u001b[0;34m(X, name)\u001b[0m\n\u001b[1;32m   1408\u001b[0m is_1d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;66;03m# check if this is iterable, except for strings which we\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;66;03m# treat as singletons.\u001b[39;00m\n\u001b[0;32m-> 1412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28miter\u001b[39m(xi)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import head_analysis_visualization\n",
    "from pathlib import Path\n",
    "\n",
    "data_directory = Path(\"./results/train/head_analysis\")\n",
    "head_analysis_visualization.create_comprehensive_visualizations(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20374933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63544797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Loading model from: ./models/ham10000/best_ham10000_vit_model_2.pth\n",
      "📋 Classes: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "📊 Dataset splits - Train: 6,409, Val: 1,603, Test: 2,003\n",
      "📁 Organizing images into ham10k/preprocessed\n",
      "📂 Found 6409 images already organized in train folder\n",
      "📋 Using existing organized images\n",
      "⚖️  Class weights:\n",
      "  akiec: 4.381\n",
      "  bcc: 2.783\n",
      "  bkl: 1.302\n",
      "  df: 12.373\n",
      "  mel: 1.286\n",
      "  nv: 0.213\n",
      "  vasc: 10.061\n",
      "🤖 Creating ViT model...\n",
      "Loading weights from ./models/ham10000/best_ham10000_vit_model_2.pth\n",
      "Checkpoint keys: ['epoch', 'model_state_dict', 'optimizer_state_dict', 'val_f1', 'class_names']\n",
      "✅ Using model_state_dict from checkpoint\n",
      "Epoch: 19\n",
      "Validation F1: 0.7720340927620811\n",
      "Classes: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "✅ Model weights loaded successfully\n",
      "Loaded ImageNet pretrained ViT-B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [00:05<00:00, 11.03it/s, Loss=0.4361, Acc=86.2%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.57      0.71      0.63        65\n",
      "         bcc       0.81      0.79      0.80       103\n",
      "         bkl       0.82      0.67      0.74       220\n",
      "          df       0.86      0.83      0.84        23\n",
      "         mel       0.63      0.67      0.65       223\n",
      "          nv       0.93      0.94      0.93      1341\n",
      "        vasc       0.81      0.89      0.85        28\n",
      "\n",
      "    accuracy                           0.86      2003\n",
      "   macro avg       0.78      0.78      0.78      2003\n",
      "weighted avg       0.86      0.86      0.86      2003\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ham10k\n",
    "\n",
    "config = ham10k.Config()\n",
    "ham10k.evaluate_saved_model(\"./models/ham10000/best_ham10000_vit_model_2.pth\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f775c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Loading model from: ./models/ham10000/best_ham10000_vit_model.pth\n",
      "📋 Classes: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "📊 Dataset splits - Train: 6,409, Val: 1,603, Test: 2,003\n",
      "📁 Organizing images into ham10k/preprocessed\n",
      "📂 Found 6409 images already organized in train folder\n",
      "📋 Using existing organized images\n",
      "⚖️  Class weights:\n",
      "  akiec: 4.381\n",
      "  bcc: 2.783\n",
      "  bkl: 1.302\n",
      "  df: 12.373\n",
      "  mel: 1.286\n",
      "  nv: 0.213\n",
      "  vasc: 10.061\n",
      "🤖 Creating ViT model...\n",
      "Loading weights from ./models/ham10000/best_ham10000_vit_model_2.pth\n",
      "Checkpoint keys: ['epoch', 'model_state_dict', 'optimizer_state_dict', 'val_f1', 'class_names']\n",
      "✅ Using model_state_dict from checkpoint\n",
      "Epoch: 19\n",
      "Validation F1: 0.7720340927620811\n",
      "Classes: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "✅ Model weights loaded successfully\n",
      "Loaded ImageNet pretrained ViT-B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 63/63 [00:05<00:00, 11.45it/s, Loss=0.5580, Acc=79.8%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.74      0.48      0.58        65\n",
      "         bcc       0.77      0.83      0.80       103\n",
      "         bkl       0.60      0.79      0.68       220\n",
      "          df       0.73      0.83      0.78        23\n",
      "         mel       0.46      0.78      0.58       223\n",
      "          nv       0.97      0.81      0.89      1341\n",
      "        vasc       0.83      0.86      0.84        28\n",
      "\n",
      "    accuracy                           0.80      2003\n",
      "   macro avg       0.73      0.77      0.73      2003\n",
      "weighted avg       0.85      0.80      0.81      2003\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ham10k\n",
    "\n",
    "config = ham10k.Config()\n",
    "ham10k.evaluate_saved_model(\"./models/ham10000/best_ham10000_vit_model.pth\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e449d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CONFIRMED_NUM_CLASSES = 6\n",
      "Loaded checkpoint with weights_only=True\n",
      "Found 'model_state_dict' in the loaded checkpoint.\n",
      "Inferred num_classes from 'lin_head.weight': 6\n",
      "Number of classes from weights matches confirmed value. Good.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to the downloaded fine-tuned weights\n",
    "weights_path = './model/vit_b-ImageNet_class_init-frozen_False-dataset_Hyperkvasir_anatomical.pth'\n",
    "# Make sure the path is correct!\n",
    "\n",
    "# --- CONFIRMED NUMBER OF CLASSES ---\n",
    "# Based on your provided table \"Anatomical landmark recognition\"\n",
    "# Cecum, Ileum, Retroflex-rectum, Pylorus, Retroflex-stomach, Z-line\n",
    "# There are 6 classes.\n",
    "CONFIRMED_NUM_CLASSES = 6\n",
    "print(f\"Using CONFIRMED_NUM_CLASSES = {CONFIRMED_NUM_CLASSES}\")\n",
    "num_classes_hyperkvasir_anatomical = CONFIRMED_NUM_CLASSES # Use this directly\n",
    "\n",
    "try:\n",
    "    # Set weights_only=True if you only want to load weights and not pickled code.\n",
    "    # However, since this is a full checkpoint, weights_only=False might be needed\n",
    "    # if their model saving relies on pickled custom classes, though less likely for state_dict.\n",
    "    # For safety, if you trust the source, weights_only=False is what you used.\n",
    "    # If you only need the state_dict, and it's just tensors, weights_only=True is safer.\n",
    "    # Let's try to be robust.\n",
    "    try:\n",
    "        loaded_checkpoint = torch.load(weights_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Loaded checkpoint with weights_only=True\")\n",
    "    except Exception as e_true:\n",
    "        print(f\"Could not load with weights_only=True ({e_true}), trying weights_only=False.\")\n",
    "        loaded_checkpoint = torch.load(weights_path, map_location='cpu', weights_only=False)\n",
    "        print(\"Loaded checkpoint with weights_only=False\")\n",
    "\n",
    "\n",
    "    actual_model_state_dict = None\n",
    "    if isinstance(loaded_checkpoint, dict) and 'model_state_dict' in loaded_checkpoint:\n",
    "        actual_model_state_dict = loaded_checkpoint['model_state_dict']\n",
    "        print(\"Found 'model_state_dict' in the loaded checkpoint.\")\n",
    "    elif isinstance(loaded_checkpoint, dict) and 'model' in loaded_checkpoint: # Fallback for other common patterns\n",
    "        actual_model_state_dict = loaded_checkpoint['model']\n",
    "        print(\"Found 'model' in the loaded checkpoint (using as state_dict).\")\n",
    "    elif isinstance(loaded_checkpoint, dict) and 'state_dict' in loaded_checkpoint: # Another fallback\n",
    "        actual_model_state_dict = loaded_checkpoint['state_dict']\n",
    "        print(\"Found 'state_dict' in the loaded checkpoint (using as state_dict).\")\n",
    "    elif isinstance(loaded_checkpoint, dict): # If it's a dict but not one of the known keys, maybe it IS the state_dict\n",
    "        actual_model_state_dict = loaded_checkpoint\n",
    "        print(\"Loaded checkpoint is a dictionary, assuming it is the state_dict directly.\")\n",
    "    else:\n",
    "        print(\"Error: Loaded checkpoint is not a dictionary or recognized structure.\")\n",
    "\n",
    "\n",
    "    if actual_model_state_dict:\n",
    "        # Their VisionTransformer_from_Any uses self.lin_head\n",
    "        # It's also possible that due to DataParallel during training, keys might be prefixed with 'module.'\n",
    "        head_weight_key_found = None\n",
    "        if 'lin_head.weight' in actual_model_state_dict:\n",
    "            head_weight_key_found = 'lin_head.weight'\n",
    "        elif 'module.lin_head.weight' in actual_model_state_dict:\n",
    "            head_weight_key_found = 'module.lin_head.weight'\n",
    "        elif 'head.weight' in actual_model_state_dict: # Fallback to standard timm naming\n",
    "            head_weight_key_found = 'head.weight'\n",
    "        elif 'module.head.weight' in actual_model_state_dict: # Fallback with module prefix\n",
    "            head_weight_key_found = 'module.head.weight'\n",
    "\n",
    "        if head_weight_key_found:\n",
    "            num_classes_from_weights = actual_model_state_dict[head_weight_key_found].shape[0]\n",
    "            print(f\"Inferred num_classes from '{head_weight_key_found}': {num_classes_from_weights}\")\n",
    "            if num_classes_from_weights != CONFIRMED_NUM_CLASSES:\n",
    "                print(f\"WARNING: Number of classes from weights ({num_classes_from_weights}) \"\n",
    "                      f\"does NOT match CONFIRMED_NUM_CLASSES ({CONFIRMED_NUM_CLASSES}). \"\n",
    "                      \"Please double-check!\")\n",
    "                # You might want to trust the weights file if there's a discrepancy\n",
    "                # num_classes_hyperkvasir_anatomical = num_classes_from_weights\n",
    "            else:\n",
    "                print(\"Number of classes from weights matches confirmed value. Good.\")\n",
    "        else:\n",
    "            print(\"Could not find 'lin_head.weight' or 'head.weight' (with or without 'module.' prefix) in the model_state_dict.\")\n",
    "            print(\"Keys found in model_state_dict (last 10):\", list(actual_model_state_dict.keys())[-10:])\n",
    "            print(f\"Proceeding with CONFIRMED_NUM_CLASSES = {CONFIRMED_NUM_CLASSES}. Ensure this is correct.\")\n",
    "    else:\n",
    "        print(\"Could not extract 'model_state_dict' for inspection.\")\n",
    "        print(f\"Proceeding with CONFIRMED_NUM_CLASSES = {CONFIRMED_NUM_CLASSES}. Ensure this is correct.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Weights file not found at {weights_path}\")\n",
    "    print(\"Please download the file and update the path.\")\n",
    "    # num_classes_hyperkvasir_anatomical is already set to CONFIRMED_NUM_CLASSES\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading or inspection: {e}\")\n",
    "    # num_classes_hyperkvasir_anatomical is already set to CONFIRMED_NUM_CLASSES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e891cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned weights from: ./model/vit_b-ImageNet_class_init-frozen_False-dataset_Hyperkvasir_anatomical.pth\n",
      "Loaded checkpoint with weights_only=False (fallback)\n",
      "Loading into SSL4GIE model structure:\n",
      "  Successfully loaded all weights into SSL4GIE model structure!\n",
      "\n",
      "Proceeding to transfer weights to your custom model.\n",
      "\n",
      "Instantiating your ViT-B model with 6 classes...\n",
      "\n",
      "Loading state_dict into your model architecture:\n",
      "  Successfully loaded all weights into your model architecture!\n",
      "\n",
      "Dummy input passed through your loaded model. Output shape: torch.Size([1, 6])\n",
      "Output shape is correct.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Assuming ssl4gie_models.py contains their VisionTransformer_from_Any class\n",
    "from ssl4gie_models import VisionTransformer_from_Any # Make sure this import works\n",
    "\n",
    "# --- Configuration ---\n",
    "NUM_CLASSES_HYPERKVASIR_ANATOMICAL = 6 # CORRECTED\n",
    "PATH_TO_FINETUNED_WEIGHTS = './model/vit_b-ImageNet_class_init-frozen_False-dataset_Hyperkvasir_anatomical.pth'\n",
    "\n",
    "# Instantiate their model structure\n",
    "ssl_model = VisionTransformer_from_Any(\n",
    "    head=True,\n",
    "    num_classes=NUM_CLASSES_HYPERKVASIR_ANATOMICAL,\n",
    "    frozen=False,\n",
    "    dense=False,\n",
    "    det=False,\n",
    "    fixed_size=224,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    out_token='cls',\n",
    "    ImageNet_weights=False\n",
    ")\n",
    "ssl_model.eval()\n",
    "\n",
    "print(f\"Loading fine-tuned weights from: {PATH_TO_FINETUNED_WEIGHTS}\")\n",
    "actual_model_state_dict_from_checkpoint = None # Initialize\n",
    "\n",
    "try:\n",
    "    try:\n",
    "        checkpoint = torch.load(PATH_TO_FINETUNED_WEIGHTS, map_location='cpu', weights_only=True)\n",
    "        print(\"Loaded checkpoint with weights_only=True\")\n",
    "    except Exception:\n",
    "        checkpoint = torch.load(PATH_TO_FINETUNED_WEIGHTS, map_location='cpu', weights_only=False)\n",
    "        print(\"Loaded checkpoint with weights_only=False (fallback)\")\n",
    "\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        actual_model_state_dict_from_checkpoint = checkpoint['model_state_dict']\n",
    "    elif isinstance(checkpoint, dict): # If it's a dict but no 'model_state_dict', assume it IS the state_dict\n",
    "         actual_model_state_dict_from_checkpoint = checkpoint\n",
    "         print(\"Checkpoint is a dict, but no 'model_state_dict' key. Assuming the checkpoint itself is the state_dict.\")\n",
    "    else:\n",
    "        print(\"ERROR: Loaded checkpoint is not a dictionary or 'model_state_dict' key not found.\")\n",
    "        raise ValueError(\"Invalid checkpoint structure\")\n",
    "\n",
    "\n",
    "    if actual_model_state_dict_from_checkpoint:\n",
    "        # Create a new state_dict without 'module.' prefix if necessary.\n",
    "        new_state_dict_for_ssl_model = {}\n",
    "        has_module_prefix = any(key.startswith('module.') for key in actual_model_state_dict_from_checkpoint.keys())\n",
    "\n",
    "        if has_module_prefix:\n",
    "            print(\"Detected 'module.' prefix in checkpoint keys. Removing it for SSL model.\")\n",
    "        for k, v in actual_model_state_dict_from_checkpoint.items():\n",
    "            name = k[7:] if has_module_prefix and k.startswith('module.') else k\n",
    "            new_state_dict_for_ssl_model[name] = v\n",
    "        \n",
    "        missing_keys, unexpected_keys = ssl_model.load_state_dict(new_state_dict_for_ssl_model, strict=False)\n",
    "        print(\"Loading into SSL4GIE model structure:\")\n",
    "        if missing_keys: print(f\"  Missing keys in SSL model: {missing_keys}\")\n",
    "        if unexpected_keys: print(f\"  Unexpected keys in SSL model: {unexpected_keys}\")\n",
    "        \n",
    "        # Check common success patterns\n",
    "        is_successful_load = True\n",
    "        if not missing_keys and not unexpected_keys:\n",
    "            print(\"  Successfully loaded all weights into SSL4GIE model structure!\")\n",
    "        elif not missing_keys and unexpected_keys == ['head.weight', 'head.bias'] and hasattr(ssl_model, 'lin_head'):\n",
    "            # This case means their VisionTransformer_from_Any base (timm.ViT) has 'head', but their subclass uses 'lin_head'.\n",
    "            # The new_state_dict_for_ssl_model would have 'lin_head' if 'module.lin_head' was in checkpoint,\n",
    "            # or 'head' if 'module.head' was in checkpoint.\n",
    "            # This specific condition might need adjustment based on actual keys.\n",
    "            print(\"  Successfully loaded backbone into SSL model. Head layer name might differ (e.g., 'head' vs 'lin_head'). This is usually okay if we map it later.\")\n",
    "        elif not unexpected_keys and all(k.startswith(\"lin_head\") for k in missing_keys) and hasattr(ssl_model, 'head') and not hasattr(ssl_model, 'lin_head'):\n",
    "            print(\" SSL model expects 'lin_head' but got 'head' or vice versa, this will be handled in mapping to your model\")\n",
    "        else:\n",
    "            print(\"  Check missing/unexpected keys during SSL4GIE model load. Some might be critical.\")\n",
    "            if missing_keys or unexpected_keys: # Be more conservative if any mismatch\n",
    "                 is_successful_load = False\n",
    "\n",
    "        if not is_successful_load:\n",
    "             print(\"  Review log for SSL model loading issues.\")\n",
    "             # Consider exiting if critical weights are missing\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: Could not extract a valid model_state_dict from the loaded checkpoint.\")\n",
    "        # exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Fine-tuned weights file not found at {PATH_TO_FINETUNED_WEIGHTS}\")\n",
    "    # exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading weights into SSL4GIE model: {e}\")\n",
    "    # exit()\n",
    "\n",
    "# Now, ssl_model contains the loaded fine-tuned weights.\n",
    "# Get its state_dict for transferring to your model if loading was successful.\n",
    "if 'is_successful_load' in locals() and is_successful_load and actual_model_state_dict_from_checkpoint is not None:\n",
    "    ssl_model_state_dict = ssl_model.state_dict() # Get the correctly formatted state_dict from ssl_model\n",
    "    print(\"\\nProceeding to transfer weights to your custom model.\")\n",
    "    # Assuming your VisionTransformer code is in 'your_vit_arch.py'\n",
    "    from translrp.ViT_new import vit_base_patch16_224 # Or your general VisionTransformer class\n",
    "\n",
    "    # Number of output classes for YOUR project.\n",
    "    # This can be the same as NUM_CLASSES_HYPERKVASIR_ANATOMICAL\n",
    "    # or different if you are adapting the fine-tuned model further.\n",
    "    # For a direct load, it should be the same.\n",
    "    NUM_CLASSES_FOR_YOUR_PROJECT = NUM_CLASSES_HYPERKVASIR_ANATOMICAL\n",
    "\n",
    "    print(f\"\\nInstantiating your ViT-B model with {NUM_CLASSES_FOR_YOUR_PROJECT} classes...\")\n",
    "    my_vit_model = vit_base_patch16_224(pretrained=False, num_classes=NUM_CLASSES_FOR_YOUR_PROJECT)\n",
    "    my_vit_model.eval()\n",
    "\n",
    "    # Prepare state_dict for your model:\n",
    "    # The main difference will be the head layer name\n",
    "    # SSL4GIE: lin_head.weight, lin_head.bias\n",
    "    # Your model: head.weight, head.bias\n",
    "    final_state_dict_for_my_model = {}\n",
    "    for key, value in ssl_model_state_dict.items():\n",
    "        if key == \"lin_head.weight\":\n",
    "            final_state_dict_for_my_model[\"head.weight\"] = value\n",
    "        elif key == \"lin_head.bias\":\n",
    "            final_state_dict_for_my_model[\"head.bias\"] = value\n",
    "        else:\n",
    "            final_state_dict_for_my_model[key] = value\n",
    "\n",
    "    print(\"\\nLoading state_dict into your model architecture:\")\n",
    "    missing_keys, unexpected_keys = my_vit_model.load_state_dict(final_state_dict_for_my_model, strict=False)\n",
    "\n",
    "    if missing_keys:\n",
    "        print(\"  Missing keys in your model (expected from SSL4GIE model):\", missing_keys)\n",
    "    if unexpected_keys:\n",
    "        print(\"  Unexpected keys in your model (not in SSL4GIE model's state_dict):\", unexpected_keys)\n",
    "\n",
    "    if not missing_keys and not unexpected_keys:\n",
    "        print(\"  Successfully loaded all weights into your model architecture!\")\n",
    "    else:\n",
    "        print(\"  Potential issues in weight loading. Review missing/unexpected keys.\")\n",
    "        print(\"  Common ViT-B keys should match. Ensure embed_dim, depth, num_heads are the same.\")\n",
    "\n",
    "    # --- Verification (Optional) ---\n",
    "    # If you have a sample input image for anatomical landmark recognition and its label\n",
    "    # you could try a forward pass.\n",
    "    # For now, a dummy input:\n",
    "    dummy_input = torch.randn(1, 3, 224, 224) # Batch of 1 image\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output = my_vit_model(dummy_input)\n",
    "        print(f\"\\nDummy input passed through your loaded model. Output shape: {output.shape}\")\n",
    "        assert output.shape == (1, NUM_CLASSES_FOR_YOUR_PROJECT)\n",
    "        print(\"Output shape is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError passing dummy input through your loaded model: {e}\")\n",
    "else:\n",
    "    print(\"\\nCritical error during SSL model weight loading. Cannot proceed to transfer to your model.\")\n",
    "    ssl_model_state_dict_for_transfer = None # Indicate failure\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceed2b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instantiating your ViT-B model with 6 classes...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ssl_model_state_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Prepare state_dict for your model:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# The main difference will be the head layer name\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# SSL4GIE: lin_head.weight, lin_head.bias\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Your model: head.weight, head.bias\u001b[39;00m\n\u001b[1;32m     18\u001b[0m final_state_dict_for_my_model \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mssl_model_state_dict\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlin_head.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     21\u001b[0m         final_state_dict_for_my_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ssl_model_state_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming your VisionTransformer code is in 'your_vit_arch.py'\n",
    "from translrp.ViT_new import vit_base_patch16_224 # Or your general VisionTransformer class\n",
    "\n",
    "# Number of output classes for YOUR project.\n",
    "# This can be the same as NUM_CLASSES_HYPERKVASIR_ANATOMICAL\n",
    "# or different if you are adapting the fine-tuned model further.\n",
    "# For a direct load, it should be the same.\n",
    "NUM_CLASSES_FOR_YOUR_PROJECT = NUM_CLASSES_HYPERKVASIR_ANATOMICAL\n",
    "\n",
    "print(f\"\\nInstantiating your ViT-B model with {NUM_CLASSES_FOR_YOUR_PROJECT} classes...\")\n",
    "my_vit_model = vit_base_patch16_224(pretrained=False, num_classes=NUM_CLASSES_FOR_YOUR_PROJECT)\n",
    "my_vit_model.eval()\n",
    "\n",
    "# Prepare state_dict for your model:\n",
    "# The main difference will be the head layer name\n",
    "# SSL4GIE: lin_head.weight, lin_head.bias\n",
    "# Your model: head.weight, head.bias\n",
    "final_state_dict_for_my_model = {}\n",
    "for key, value in ssl_model_state_dict.items():\n",
    "    if key == \"lin_head.weight\":\n",
    "        final_state_dict_for_my_model[\"head.weight\"] = value\n",
    "    elif key == \"lin_head.bias\":\n",
    "        final_state_dict_for_my_model[\"head.bias\"] = value\n",
    "    else:\n",
    "        final_state_dict_for_my_model[key] = value\n",
    "\n",
    "print(\"\\nLoading state_dict into your model architecture:\")\n",
    "missing_keys, unexpected_keys = my_vit_model.load_state_dict(final_state_dict_for_my_model, strict=False)\n",
    "\n",
    "if missing_keys:\n",
    "    print(\"  Missing keys in your model (expected from SSL4GIE model):\", missing_keys)\n",
    "if unexpected_keys:\n",
    "    print(\"  Unexpected keys in your model (not in SSL4GIE model's state_dict):\", unexpected_keys)\n",
    "\n",
    "if not missing_keys and not unexpected_keys:\n",
    "    print(\"  Successfully loaded all weights into your model architecture!\")\n",
    "else:\n",
    "    print(\"  Potential issues in weight loading. Review missing/unexpected keys.\")\n",
    "    print(\"  Common ViT-B keys should match. Ensure embed_dim, depth, num_heads are the same.\")\n",
    "\n",
    "# --- Verification (Optional) ---\n",
    "# If you have a sample input image for anatomical landmark recognition and its label\n",
    "# you could try a forward pass.\n",
    "# For now, a dummy input:\n",
    "dummy_input = torch.randn(1, 3, 224, 224) # Batch of 1 image\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        output = my_vit_model(dummy_input)\n",
    "    print(f\"\\nDummy input passed through your loaded model. Output shape: {output.shape}\")\n",
    "    assert output.shape == (1, NUM_CLASSES_FOR_YOUR_PROJECT)\n",
    "    print(\"Output shape is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError passing dummy input through your loaded model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d82c8937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting weight transformation ---\n",
      "Input checkpoint: ./model/vit_b-ImageNet_class_init-frozen_False-dataset_Hyperkvasir_anatomical.pth\n",
      "Output for transformed weights: ./model/vit_b_hyperkvasir_anatomical_for_translrp.pth\n",
      "Number of classes: 6\n",
      "Loaded checkpoint with weights_only=False (fallback)\n",
      "Successfully loaded weights into intermediate SSL4GIE model structure.\n",
      "Instantiating your target ViT-B model with 6 classes...\n",
      "Successfully loaded transformed weights into your target model architecture!\n",
      "Successfully saved transformed model state_dict to: ./model/vit_b_hyperkvasir_anatomical_for_translrp.pth\n",
      "--- Weight transformation complete ---\n",
      "\n",
      "--- Testing loading the newly saved transformed weights ---\n",
      "Successfully loaded saved transformed weights into a new instance of your model.\n",
      "Dummy input passed through test_model. Output shape: torch.Size([1, 6])\n",
      "Output shape is correct.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import VisionTransformer as TimmVisionTransformer\n",
    "from translrp.ViT_new import vit_base_patch16_224\n",
    "\n",
    "# --- SSL4GIE's VisionTransformer_from_Any (Simplified for this use case) ---\n",
    "# We only need the parts relevant to head=True, dense=False, det=False\n",
    "class SSL4GIE_ViT(TimmVisionTransformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        # Unused for this specific loading but kept for signature compatibility if strict loading was used\n",
    "        head=True, \n",
    "        frozen=False,\n",
    "        dense=False,\n",
    "        det=False,\n",
    "        fixed_size=224,\n",
    "        out_token='cls',\n",
    "        ImageNet_weights=False, # Will be False when loading their fine-tuned checkpoint\n",
    "    ):\n",
    "        super().__init__(\n",
    "            patch_size=16, embed_dim=embed_dim, depth=depth, num_heads=num_heads, num_classes=0 # Base timm ViT's head is Identity if num_classes=0\n",
    "        )\n",
    "        # Original SSL4GIE model sets self.head = nn.Identity() and then adds self.lin_head\n",
    "        # Replicating that structure for faithful intermediate loading:\n",
    "        self.head = nn.Identity() # This is the timm.ViT's head\n",
    "        self.lin_head = nn.Linear(embed_dim, num_classes) # This is SSL4GIE's specific head\n",
    "        \n",
    "        self.head_bool = head # For their forward logic\n",
    "        self.out_token = out_token # For their forward logic\n",
    "\n",
    "        # Unused for this specific task but part of their class structure\n",
    "        self.frozen = frozen\n",
    "        self.dense = dense\n",
    "        self.det = det\n",
    "        if ImageNet_weights: # This path won't be taken when loading their fine-tuned ckpt\n",
    "            # This is a placeholder, as the actual download URL is for their ImageNet_class init\n",
    "            # which we are bypassing by loading their finetuned weights directly.\n",
    "            print(\"WARNING: ImageNet_weights=True in SSL4GIE_ViT init, but we load custom weights.\")\n",
    "\n",
    "\n",
    "    def forward(self, x): # Simplified forward for classification\n",
    "        x = self.forward_features(x) # From TimmVisionTransformer\n",
    "        if self.out_token == \"cls\":\n",
    "            x = x[:, 0]\n",
    "        elif self.out_token == \"spatial\": # Not typical for their classification ViT-B\n",
    "            x = x[:, 1:].mean(1)\n",
    "        # If self.head_bool is True (which it is for classification)\n",
    "        x = self.lin_head(x) # Use their specific linear head\n",
    "        return x\n",
    "\n",
    "\n",
    "def transform_and_save_weights(\n",
    "    input_checkpoint_path,\n",
    "    output_weights_path,\n",
    "    num_classes,\n",
    "    verbose=True):\n",
    "    \"\"\"\n",
    "    Loads weights from an SSL4GIE checkpoint, transforms them for a\n",
    "    translrp.ViT_new architecture, and saves the transformed state_dict.\n",
    "\n",
    "    Args:\n",
    "        input_checkpoint_path (str): Path to the SSL4GIE .pth checkpoint file.\n",
    "        output_weights_path (str): Path to save the transformed model's state_dict.\n",
    "        num_classes (int): Number of output classes for the final model.\n",
    "        verbose (bool): Whether to print detailed loading messages.\n",
    "    \"\"\"\n",
    "    if verbose: print(f\"--- Starting weight transformation ---\")\n",
    "    if verbose: print(f\"Input checkpoint: {input_checkpoint_path}\")\n",
    "    if verbose: print(f\"Output for transformed weights: {output_weights_path}\")\n",
    "    if verbose: print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    # 1. Instantiate SSL4GIE's model structure\n",
    "    ssl_temp_model = SSL4GIE_ViT(\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=768, depth=12, num_heads=12, # Standard ViT-B params\n",
    "        # Other params are defaults or not affecting weight loading structure for this case\n",
    "    )\n",
    "    ssl_temp_model.eval()\n",
    "\n",
    "    # 2. Load the downloaded fine-tuned weights into ssl_temp_model\n",
    "    actual_model_state_dict_from_checkpoint = None\n",
    "    try:\n",
    "        try:\n",
    "            checkpoint = torch.load(input_checkpoint_path, map_location='cpu', weights_only=True)\n",
    "            if verbose: print(\"Loaded checkpoint with weights_only=True\")\n",
    "        except Exception:\n",
    "            checkpoint = torch.load(input_checkpoint_path, map_location='cpu', weights_only=False)\n",
    "            if verbose: print(\"Loaded checkpoint with weights_only=False (fallback)\")\n",
    "\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            actual_model_state_dict_from_checkpoint = checkpoint['model_state_dict']\n",
    "        elif isinstance(checkpoint, dict):\n",
    "            actual_model_state_dict_from_checkpoint = checkpoint\n",
    "            if verbose: print(\"Checkpoint is a dict, but no 'model_state_dict' key. Assuming it's the state_dict.\")\n",
    "        else:\n",
    "            raise ValueError(\"Loaded checkpoint is not a dictionary or 'model_state_dict' key not found.\")\n",
    "\n",
    "        if actual_model_state_dict_from_checkpoint:\n",
    "            new_state_dict_for_ssl_model = {}\n",
    "            has_module_prefix = any(key.startswith('module.') for key in actual_model_state_dict_from_checkpoint.keys())\n",
    "            if has_module_prefix and verbose:\n",
    "                print(\"Detected 'module.' prefix in checkpoint. Removing it.\")\n",
    "            for k, v in actual_model_state_dict_from_checkpoint.items():\n",
    "                name = k[7:] if has_module_prefix and k.startswith('module.') else k\n",
    "                new_state_dict_for_ssl_model[name] = v\n",
    "            \n",
    "            ssl_temp_model.load_state_dict(new_state_dict_for_ssl_model, strict=True) # Be strict here\n",
    "            if verbose: print(\"Successfully loaded weights into intermediate SSL4GIE model structure.\")\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract a valid model_state_dict from the checkpoint.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading weights into intermediate SSL4GIE model: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Get the state_dict from the populated ssl_temp_model\n",
    "    ssl_model_clean_state_dict = ssl_temp_model.state_dict()\n",
    "\n",
    "    # 4. Instantiate your target model structure\n",
    "    if verbose: print(f\"Instantiating your target ViT-B model with {num_classes} classes...\")\n",
    "    target_model = vit_base_patch16_224(pretrained=False, num_classes=num_classes)\n",
    "    target_model.eval()\n",
    "\n",
    "    # 5. Prepare state_dict for your target model (mapping SSL4GIE's 'lin_head' to your 'head')\n",
    "    final_state_dict_for_target_model = {}\n",
    "    for key, value in ssl_model_clean_state_dict.items():\n",
    "        if key == \"lin_head.weight\": # SSL4GIE_ViT uses lin_head\n",
    "            final_state_dict_for_target_model[\"head.weight\"] = value # Your ViT_new uses head\n",
    "        elif key == \"lin_head.bias\":\n",
    "            final_state_dict_for_target_model[\"head.bias\"] = value\n",
    "        else:\n",
    "            final_state_dict_for_target_model[key] = value\n",
    "    \n",
    "    # 6. Load into your target model\n",
    "    try:\n",
    "        target_model.load_state_dict(final_state_dict_for_target_model, strict=True) # Be strict\n",
    "        if verbose: print(\"Successfully loaded transformed weights into your target model architecture!\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading transformed weights into your target model: {e}\")\n",
    "        # You might want to try strict=False here for debugging, but True is preferred for final check\n",
    "        # missing_keys, unexpected_keys = target_model.load_state_dict(final_state_dict_for_target_model, strict=False)\n",
    "        # print(f\"  Missing keys: {missing_keys}\")\n",
    "        # print(f\"  Unexpected keys: {unexpected_keys}\")\n",
    "        return\n",
    "\n",
    "    # 7. Save the state_dict of your transformed model\n",
    "    try:\n",
    "        torch.save(target_model.state_dict(), output_weights_path)\n",
    "        if verbose: print(f\"Successfully saved transformed model state_dict to: {output_weights_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving transformed model state_dict: {e}\")\n",
    "        return\n",
    "\n",
    "    if verbose: print(f\"--- Weight transformation complete ---\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    NUM_CLASSES = 6 # For Hyperkvasir anatomical landmarks\n",
    "    \n",
    "    # Path to the downloaded SSL4GIE fine-tuned weights\n",
    "    INPUT_SSL4GIE_CHECKPOINT_PATH = './model/vit_b-ImageNet_class_init-frozen_False-dataset_Hyperkvasir_anatomical.pth'\n",
    "    \n",
    "    # Path where you want to save the transformed weights compatible with your translrp.ViT_new\n",
    "    OUTPUT_TRANSFORMED_WEIGHTS_PATH = './model/vit_b_hyperkvasir_anatomical_for_translrp.pth'\n",
    "\n",
    "    transform_and_save_weights(\n",
    "        input_checkpoint_path=INPUT_SSL4GIE_CHECKPOINT_PATH,\n",
    "        output_weights_path=OUTPUT_TRANSFORMED_WEIGHTS_PATH,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # --- Optional: Test loading the saved transformed weights ---\n",
    "    print(\"\\n--- Testing loading the newly saved transformed weights ---\")\n",
    "    try:\n",
    "        test_model = vit_base_patch16_224(pretrained=False, num_classes=NUM_CLASSES)\n",
    "        test_model.load_state_dict(torch.load(OUTPUT_TRANSFORMED_WEIGHTS_PATH, map_location='cpu'))\n",
    "        test_model.eval()\n",
    "        print(f\"Successfully loaded saved transformed weights into a new instance of your model.\")\n",
    "        \n",
    "        # Dummy input test\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            output = test_model(dummy_input)\n",
    "        print(f\"Dummy input passed through test_model. Output shape: {output.shape}\")\n",
    "        assert output.shape == (1, NUM_CLASSES)\n",
    "        print(\"Output shape is correct.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Test load failed: Transformed weights file not found at {OUTPUT_TRANSFORMED_WEIGHTS_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test load failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3d30cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Running evaluation with the following mapping from vit.model.py ---\n",
      "CLASSES: ['cecum', 'ileum', 'pylorus', 'retroflex-rectum', 'retroflex-stomach', 'z-line']\n",
      "CLS2IDX: {0: 'cecum', 1: 'ileum', 2: 'pylorus', 3: 'retroflex-rectum', 4: 'retroflex-stomach', 5: 'z-line'}\n",
      "IDX2CLS: {'cecum': 0, 'ileum': 1, 'pylorus': 2, 'retroflex-rectum': 3, 'retroflex-stomach': 4, 'z-line': 5}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PipelineConfig' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 200\u001b[0m\n\u001b[1;32m    196\u001b[0m NUM_MODEL_CLASSES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# The script will now use whatever CLASSES, CLS2IDX, IDX2CLS are currently\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# defined in vit/model.py when it's imported.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mrun_evaluation_with_current_mapping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_MODEL_CLASSES\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo test a different mapping, edit vit/model.py (CLASSES, CLS2IDX, IDX2CLS) and re-run this script.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 121\u001b[0m, in \u001b[0;36mrun_evaluation_with_current_mapping\u001b[0;34m(config, model_path, num_classes)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDX2CLS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_module\u001b[38;5;241m.\u001b[39mIDX2CLS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# ---\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Create data loaders - they will pick up the current mappings from vit.model\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Assuming batch_size is defined, e.g., config.eval.batch_size or hardcoded\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m eval_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m16\u001b[39m) \u001b[38;5;66;03m# Example: get from config or default to 16\u001b[39;00m\n\u001b[1;32m    122\u001b[0m _, _, test_loader, class_names_for_report \u001b[38;5;241m=\u001b[39m create_data_loaders(config, batch_size\u001b[38;5;241m=\u001b[39meval_batch_size)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrder for classification_report: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names_for_report\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelineConfig' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score # For evaluation\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# Assuming your vit.preprocessing and other imports are available\n",
    "import vit.preprocessing as preprocessing\n",
    "# Import your model loading and PipelineConfig\n",
    "import vit.model as model_module # Assuming vit.model contains your CLASSES, CLS2IDX, IDX2CLS and model loading\n",
    "from config import PipelineConfig\n",
    "\n",
    "\n",
    "# --- Modified HyperkvasirDataset ---\n",
    "# This version will use the CLS2IDX imported from vit.model for the current run\n",
    "class HyperkvasirDataset(Dataset):\n",
    "    def __init__(self, image_dir, current_cls2idx, current_idx2cls, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_paths = sorted(list(self.image_dir.glob(\"*.jpg\")))\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Use the mappings provided for THIS RUN\n",
    "        self.cls_to_idx = current_cls2idx\n",
    "        self.idx_to_cls = current_idx2cls # For potentially deriving ordered class names\n",
    "        \n",
    "        # Derive class_names for reports based on the numerical order of indices in idx_to_cls\n",
    "        # This ensures the report labels match the 0-5 order of the model output\n",
    "        self.class_names_for_report = [self.idx_to_cls[i] for i in range(len(self.idx_to_cls))]\n",
    "\n",
    "        if not self.image_paths:\n",
    "            print(f\"Warning: No images found in {self.image_dir}\")\n",
    "        self._verify_filename_suffixes() # Important sanity check\n",
    "\n",
    "    def _verify_filename_suffixes(self):\n",
    "        unmappable_files = []\n",
    "        for path in self.image_paths:\n",
    "            class_name_from_file = path.stem.split('_')[-1]\n",
    "            if class_name_from_file not in self.cls_to_idx:\n",
    "                unmappable_files.append(path)\n",
    "        if unmappable_files:\n",
    "            print(f\"ERROR: The following files have suffixes that cannot be mapped using the current CLS2IDX:\")\n",
    "            for f_path in unmappable_files[:5]:\n",
    "                print(f\"  - {f_path} (suffix: {f_path.stem.split('_')[-1]})\")\n",
    "            if len(unmappable_files) > 5:\n",
    "                print(f\"  ... and {len(unmappable_files) - 5} more.\")\n",
    "            print(f\"Keys in current CLS2IDX: {list(self.cls_to_idx.keys())}\")\n",
    "            raise ValueError(\"Mismatch between filename suffixes and CLS2IDX keys. Check vit/model.py or your manual definition.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        class_name_from_file = image_path.stem.split('_')[-1]\n",
    "        \n",
    "        # This check should pass if _verify_filename_suffixes passed\n",
    "        if class_name_from_file not in self.cls_to_idx:\n",
    "             raise ValueError(f\"Class '{class_name_from_file}' from file {image_path} not in CLS2IDX map.\")\n",
    "        \n",
    "        class_idx_numerical = self.cls_to_idx[class_name_from_file]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, class_idx_numerical, str(image_path) # Return path for debugging if needed\n",
    "\n",
    "# --- Modified create_data_loaders ---\n",
    "# It will now implicitly use the mappings imported from vit.model at the time of the call\n",
    "def create_data_loaders(config, batch_size=32, source_dir_root: Path = Path(\"./hyper-kvasir/preprocessed/\")):\n",
    "    # These will be the mappings currently defined in vit.model when this function is called\n",
    "    current_classes_list = model_module.CLASSES\n",
    "    current_cls2idx = model_module.CLS2IDX\n",
    "    current_idx2cls = model_module.IDX2CLS\n",
    "\n",
    "    # Sanity check: Ensure CLS2IDX and IDX2CLS are consistent with CLASSES\n",
    "    if len(current_classes_list) != len(current_cls2idx) or len(current_classes_list) != len(current_idx2cls):\n",
    "        raise ValueError(\"Mismatch in lengths of CLASSES, CLS2IDX, IDX2CLS in vit.model.py\")\n",
    "    for i, name in enumerate(current_classes_list):\n",
    "        if current_cls2idx.get(name) != i or current_idx2cls.get(i) != name:\n",
    "            raise ValueError(f\"Inconsistency in mappings for class '{name}' at index {i} in vit.model.py. \"\n",
    "                             f\"Expected CLS2IDX['{name}'] == {i} and IDX2CLS[{i}] == '{name}'. \"\n",
    "                             f\"Got CLS2IDX.get('{name}') = {current_cls2idx.get(name)}, IDX2CLS.get({i}) = {current_idx2cls.get(i)}\")\n",
    "\n",
    "\n",
    "    processor = preprocessing.get_processor_for_precached_224_images()\n",
    "\n",
    "    train_dataset = HyperkvasirDataset(source_dir_root / \"train\", current_cls2idx, current_idx2cls, transform=processor)\n",
    "    val_dataset = HyperkvasirDataset(source_dir_root / \"val\", current_cls2idx, current_idx2cls, transform=processor)\n",
    "    test_dataset = HyperkvasirDataset(source_dir_root / \"test\", current_cls2idx, current_idx2cls, transform=processor)\n",
    "\n",
    "    class_names_for_report = test_dataset.class_names_for_report\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, class_names_for_report\n",
    "\n",
    "\n",
    "# --- Your main evaluation function (train_vit_on_hyperkvasir or equivalent) ---\n",
    "# This is where you'll call create_data_loaders and the evaluation logic\n",
    "def run_evaluation_with_current_mapping(config, model_path, num_classes):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Crucial: Print the mapping being used for this run ---\n",
    "    print(f\"\\n--- Running evaluation with the following mapping from vit.model.py ---\")\n",
    "    print(f\"CLASSES: {model_module.CLASSES}\")\n",
    "    print(f\"CLS2IDX: {model_module.CLS2IDX}\")\n",
    "    print(f\"IDX2CLS: {model_module.IDX2CLS}\")\n",
    "    # ---\n",
    "\n",
    "    # Create data loaders - they will pick up the current mappings from vit.model\n",
    "    # Assuming batch_size is defined, e.g., config.eval.batch_size or hardcoded\n",
    "    eval_batch_size = getattr(config.eval, 'batch_size', 16) # Example: get from config or default to 16\n",
    "    _, _, test_loader, class_names_for_report = create_data_loaders(config, batch_size=eval_batch_size)\n",
    "    \n",
    "    print(f\"Order for classification_report: {class_names_for_report}\")\n",
    "\n",
    "    # Load your ViT model\n",
    "    # This assumes model_module.load_vit_model uses the num_classes and loads weights properly\n",
    "    vit_model = model_module.load_vit_model(device=device, model_path=model_path, num_classes=num_classes).to(device)\n",
    "    vit_model.eval()\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss() # Not strictly needed if only evaluating, but good practice\n",
    "\n",
    "    # Detailed test evaluation loop (similar to your validate_epoch)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vit_model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nDetailed Test Results for the current mapping:\")\n",
    "    # The class_names_for_report from create_data_loaders ensures labels in the report match the model's output indices\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names_for_report, zero_division=0))\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
    "    print(f\"--- Evaluation with current mapping complete ---\\n\")\n",
    "\n",
    "\n",
    "# --- How you'd use it ---\n",
    "# In your main script (e.g., the one with if __name__ == \"__main__\":)\n",
    "\n",
    "# 1. BEFORE RUNNING: Manually edit vit/model.py to set the desired order in CLASSES.\n",
    "#    Then ensure CLS2IDX and IDX2CLS are correctly derived from it.\n",
    "#    Example content of vit/model.py:\n",
    "#    ```python\n",
    "#    # In vit/model.py\n",
    "#    # Experiment 1: Alphabetical (default)\n",
    "#    CLASSES = [\"cecum\", \"ileum\", \"pylorus\", \"retroflex-rectum\", \"retroflex-stomach\", \"z-line\"]\n",
    "#    # Experiment 2: Table Order\n",
    "#    #CLASSES = [\"cecum\", \"ileum\", \"retroflex-rectum\", \"pylorus\", \"retroflex-stomach\", \"z-line\"]\n",
    "#\n",
    "#    CLS2IDX = {cls: i for i, cls in enumerate(CLASSES)} # Corrected: class name to index\n",
    "#    IDX2CLS = {i: cls for i, cls in enumerate(CLASSES)} # Corrected: index to class name\n",
    "#\n",
    "#    # ... your load_vit_model function and ViT class definition ...\n",
    "#    def load_vit_model(device, model_path, num_classes):\n",
    "#        # ... your actual model instantiation and weight loading ...\n",
    "#        from translrp.ViT_new import vit_base_patch16_224 # Assuming this is your model class\n",
    "#        m = vit_base_patch16_224(pretrained=False, num_classes=num_classes)\n",
    "#        m.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#        return m\n",
    "#    ```\n",
    "#\n",
    "# 2. Then run your main evaluation script:\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    pipeline_config = PipelineConfig() # Load your configuration\n",
    "    \n",
    "    # Ensure translrp.ViT_new is importable by adding its parent to sys.path if necessary\n",
    "    import sys\n",
    "    import os\n",
    "    # Example: if translrp is in the project root and this script is in a subfolder\n",
    "    # SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) # e.g. /path/to/project/scripts\n",
    "    # PROJECT_ROOT = os.path.dirname(SCRIPT_DIR) # e.g. /path/to/project\n",
    "    # if PROJECT_ROOT not in sys.path:\n",
    "    #    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "    # Path to your transformed model weights\n",
    "    MODEL_PATH = './model/vit_b_hyperkvasir_anatomical_for_translrp.pth'\n",
    "    NUM_MODEL_CLASSES = 6\n",
    "\n",
    "    # The script will now use whatever CLASSES, CLS2IDX, IDX2CLS are currently\n",
    "    # defined in vit/model.py when it's imported.\n",
    "    run_evaluation_with_current_mapping(\n",
    "        config=pipeline_config,\n",
    "        model_path=MODEL_PATH,\n",
    "        num_classes=NUM_MODEL_CLASSES\n",
    "    )\n",
    "\n",
    "    print(\"To test a different mapping, edit vit/model.py (CLASSES, CLS2IDX, IDX2CLS) and re-run this script.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
