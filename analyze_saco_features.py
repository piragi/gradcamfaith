"""
Comprehensive Analysis of SAE Feature Dictionaries

This script analyzes the feature dictionaries generated by saco_feature_analysis_v2.py
to understand patterns in under-attributed and over-attributed features.
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, Any, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")


def load_feature_results(file_path: str) -> Dict[str, Any]:
    """Load saved feature analysis results."""
    print(f"\nLoading results from: {file_path}")
    results = torch.load(file_path, map_location='cpu', weights_only=False)
    return results


def basic_statistics(results: Dict[str, Any]) -> Dict[str, Any]:
    """Extract basic statistics from the feature dictionaries."""
    
    stats = {}
    
    # Get the feature dictionaries
    under_attr = results['results_by_type']['under_attributed']
    over_attr = results['results_by_type']['over_attributed']
    
    # Basic counts
    stats['n_under_attributed'] = len(under_attr)
    stats['n_over_attributed'] = len(over_attr)
    stats['total_features'] = stats['n_under_attributed'] + stats['n_over_attributed']
    
    # Analysis parameters
    if 'analysis_params' in results:
        stats['params'] = results['analysis_params']
    
    return stats, under_attr, over_attr


def analyze_distributions(features_dict: Dict[int, Dict[str, Any]], category: str) -> pd.DataFrame:
    """Analyze the distributions of various metrics for a feature category."""
    
    if not features_dict:
        return pd.DataFrame()
    
    data = []
    for feat_id, feat_stats in features_dict.items():
        data.append({
            'feature_id': feat_id,
            'mean_log_ratio': feat_stats['mean_log_ratio'],
            'std_log_ratio': feat_stats.get('std_log_ratio', 0),
            'n_occurrences': feat_stats['n_occurrences'],
            'confidence_score': feat_stats.get('confidence_score', 0),
            'balanced_score': feat_stats.get('balanced_score', 0),
            'sum_of_means': feat_stats.get('sum_of_means', 0),
            'sum_of_sums': feat_stats.get('sum_of_sums', 0),
            'avg_n_patches': feat_stats.get('avg_n_patches', 0),
            'avg_strength': feat_stats.get('avg_strength', 0),
            'dominant_class': feat_stats.get('dominant_class', 'unknown'),
            'category': category
        })
    
    return pd.DataFrame(data)


def analyze_class_patterns(features_dict: Dict[int, Dict[str, Any]], category: str) -> Dict[str, Any]:
    """Analyze class-specific patterns in features."""
    
    class_stats = defaultdict(lambda: {
        'n_features': 0,
        'total_occurrences': 0,
        'dominant_count': 0,
        'feature_ids': [],
        'mean_log_ratios': [],
        'confidence_scores': []
    })
    
    for feat_id, feat_stats in features_dict.items():
        # Get class distribution
        class_dist = feat_stats.get('class_distribution', {})
        dominant_class = feat_stats.get('dominant_class', 'unknown')
        
        # Update statistics for each class
        for class_name, count in class_dist.items():
            class_stats[class_name]['n_features'] += 1
            class_stats[class_name]['total_occurrences'] += count
            if class_name == dominant_class:
                class_stats[class_name]['dominant_count'] += 1
            class_stats[class_name]['feature_ids'].append(feat_id)
            class_stats[class_name]['mean_log_ratios'].append(feat_stats['mean_log_ratio'])
            class_stats[class_name]['confidence_scores'].append(feat_stats.get('confidence_score', 0))
    
    # Calculate aggregated statistics
    for class_name in class_stats:
        stats = class_stats[class_name]
        if stats['mean_log_ratios']:
            stats['avg_mean_log_ratio'] = np.mean(stats['mean_log_ratios'])
            stats['std_mean_log_ratio'] = np.std(stats['mean_log_ratios'])
            stats['avg_confidence'] = np.mean(stats['confidence_scores'])
        else:
            stats['avg_mean_log_ratio'] = 0
            stats['std_mean_log_ratio'] = 0
            stats['avg_confidence'] = 0
        
        # Clean up lists for summary
        del stats['mean_log_ratios']
        del stats['confidence_scores']
        stats['sample_feature_ids'] = stats['feature_ids'][:10]  # Keep only sample
        del stats['feature_ids']
    
    return dict(class_stats)


def print_analysis_report(results_path: str):
    """Generate a comprehensive analysis report."""
    
    # Load results
    results = load_feature_results(results_path)
    stats, under_attr, over_attr = basic_statistics(results)
    
    print("\n" + "="*80)
    print("FEATURE ANALYSIS REPORT")
    print("="*80)
    
    # Basic Statistics
    print("\n1. BASIC STATISTICS")
    print("-"*40)
    print(f"Total features identified: {stats['total_features']}")
    print(f"  - Under-attributed: {stats['n_under_attributed']} ({100*stats['n_under_attributed']/max(1,stats['total_features']):.1f}%)")
    print(f"  - Over-attributed: {stats['n_over_attributed']} ({100*stats['n_over_attributed']/max(1,stats['total_features']):.1f}%)")
    
    if 'params' in stats:
        print(f"\nAnalysis Parameters:")
        for key, value in stats['params'].items():
            print(f"  - {key}: {value}")
    
    # Analyze distributions for both categories
    print("\n2. FEATURE DISTRIBUTIONS")
    print("-"*40)
    
    for category, features in [('Under-attributed', under_attr), ('Over-attributed', over_attr)]:
        if not features:
            continue
            
        df = analyze_distributions(features, category)
        
        print(f"\n{category.upper()} Features:")
        print(f"  Total: {len(df)}")
        
        # Occurrence statistics
        print(f"\n  Occurrence Statistics:")
        print(f"    - Mean: {df['n_occurrences'].mean():.1f}")
        print(f"    - Median: {df['n_occurrences'].median():.1f}")
        print(f"    - Min: {df['n_occurrences'].min()}")
        print(f"    - Max: {df['n_occurrences'].max()}")
        print(f"    - Std: {df['n_occurrences'].std():.1f}")
        
        # Log ratio statistics
        print(f"\n  Log Ratio Statistics:")
        print(f"    - Mean: {df['mean_log_ratio'].mean():.3f}")
        print(f"    - Median: {df['mean_log_ratio'].median():.3f}")
        print(f"    - Min: {df['mean_log_ratio'].min():.3f}")
        print(f"    - Max: {df['mean_log_ratio'].max():.3f}")
        print(f"    - Std: {df['mean_log_ratio'].std():.3f}")
        
        # Confidence score statistics
        if 'confidence_score' in df.columns:
            print(f"\n  Confidence Score Statistics:")
            print(f"    - Mean: {df['confidence_score'].mean():.3f}")
            print(f"    - Median: {df['confidence_score'].median():.3f}")
            print(f"    - Min: {df['confidence_score'].min():.3f}")
            print(f"    - Max: {df['confidence_score'].max():.3f}")
        
        # Top features
        print(f"\n  Top 5 Features by Confidence Score:")
        top_features = df.nlargest(5, 'confidence_score')
        for idx, row in top_features.iterrows():
            print(f"    Feature {int(row['feature_id']):5d}: conf={row['confidence_score']:.3f}, "
                  f"n_occ={int(row['n_occurrences']):3d}, log_ratio={row['mean_log_ratio']:+.3f}, "
                  f"class={row['dominant_class']}")
    
    # Class-specific analysis
    print("\n3. CLASS-SPECIFIC PATTERNS")
    print("-"*40)
    
    for category, features in [('Under-attributed', under_attr), ('Over-attributed', over_attr)]:
        if not features:
            continue
            
        print(f"\n{category.upper()} Features by Class:")
        class_stats = analyze_class_patterns(features, category)
        
        # Sort by total occurrences
        sorted_classes = sorted(class_stats.items(), 
                               key=lambda x: x[1]['total_occurrences'], 
                               reverse=True)
        
        for class_name, stats in sorted_classes:
            print(f"\n  {class_name}:")
            print(f"    - Features appearing in this class: {stats['n_features']}")
            print(f"    - Total occurrences: {stats['total_occurrences']}")
            print(f"    - Features where this is dominant: {stats['dominant_count']}")
            if stats['avg_mean_log_ratio'] != 0:
                print(f"    - Avg log ratio: {stats['avg_mean_log_ratio']:.3f} Â± {stats['std_mean_log_ratio']:.3f}")
                print(f"    - Avg confidence: {stats['avg_confidence']:.3f}")
    
    # Feature overlap analysis
    print("\n4. FEATURE FREQUENCY ANALYSIS")
    print("-"*40)
    
    for category, features in [('Under-attributed', under_attr), ('Over-attributed', over_attr)]:
        if not features:
            continue
            
        df = analyze_distributions(features, category)
        
        # Binned occurrence analysis
        bins = [0, 10, 20, 50, 100, 500, 1000, float('inf')]
        labels = ['1-10', '11-20', '21-50', '51-100', '101-500', '501-1000', '1000+']
        df['occ_bin'] = pd.cut(df['n_occurrences'], bins=bins, labels=labels, include_lowest=True)
        
        print(f"\n{category.upper()} - Feature Frequency Distribution:")
        occ_dist = df['occ_bin'].value_counts().sort_index()
        for bin_label, count in occ_dist.items():
            pct = 100 * count / len(df)
            print(f"  {bin_label:10s}: {count:4d} features ({pct:5.1f}%)")
    
    # Ratio distribution analysis  
    print("\n5. LOG RATIO DISTRIBUTION ANALYSIS")
    print("-"*40)
    
    for category, features in [('Under-attributed', under_attr), ('Over-attributed', over_attr)]:
        if not features:
            continue
            
        df = analyze_distributions(features, category)
        
        # Create ratio bins
        if category == 'Under-attributed':
            ratio_bins = [0, 0.5, 1.0, 1.5, 2.0, 3.0, float('inf')]
            ratio_labels = ['0-0.5', '0.5-1.0', '1.0-1.5', '1.5-2.0', '2.0-3.0', '3.0+']
        else:
            ratio_bins = [-float('inf'), -3.0, -2.0, -1.5, -1.0, -0.5, 0]
            ratio_labels = ['< -3.0', '-3.0 to -2.0', '-2.0 to -1.5', '-1.5 to -1.0', '-1.0 to -0.5', '-0.5 to 0']
        
        df['ratio_bin'] = pd.cut(df['mean_log_ratio'], bins=ratio_bins, labels=ratio_labels, include_lowest=True)
        
        print(f"\n{category.upper()} - Log Ratio Distribution:")
        ratio_dist = df['ratio_bin'].value_counts().sort_index()
        for bin_label, count in ratio_dist.items():
            pct = 100 * count / len(df)
            print(f"  {bin_label:15s}: {count:4d} features ({pct:5.1f}%)")
    
    print("\n" + "="*80)
    print("END OF REPORT")
    print("="*80)


def create_visualizations(results_path: str, save_dir: str = "analysis_plots"):
    """Create and save visualization plots."""
    
    # Create save directory
    save_path = Path(save_dir)
    save_path.mkdir(exist_ok=True)
    
    # Load results
    results = load_feature_results(results_path)
    _, under_attr, over_attr = basic_statistics(results)
    
    # Prepare dataframes
    df_under = analyze_distributions(under_attr, 'under_attributed')
    df_over = analyze_distributions(over_attr, 'over_attributed')
    df_all = pd.concat([df_under, df_over], ignore_index=True)
    
    # Create figure with subplots
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('SAE Feature Analysis Visualizations', fontsize=16, y=1.02)
    
    # 1. Feature count comparison
    ax = axes[0, 0]
    categories = ['Under-attributed', 'Over-attributed']
    counts = [len(df_under), len(df_over)]
    colors = ['#ff7f0e', '#2ca02c']
    bars = ax.bar(categories, counts, color=colors, alpha=0.7)
    ax.set_ylabel('Number of Features')
    ax.set_title('Feature Count by Category')
    for bar, count in zip(bars, counts):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                str(count), ha='center', va='bottom', fontweight='bold')
    
    # 2. Occurrence distribution
    ax = axes[0, 1]
    if not df_under.empty:
        ax.hist(df_under['n_occurrences'], bins=30, alpha=0.5, label='Under-attr', color='#ff7f0e')
    if not df_over.empty:
        ax.hist(df_over['n_occurrences'], bins=30, alpha=0.5, label='Over-attr', color='#2ca02c')
    ax.set_xlabel('Number of Occurrences')
    ax.set_ylabel('Number of Features')
    ax.set_title('Feature Occurrence Distribution')
    ax.legend()
    ax.set_yscale('log')
    
    # 3. Log ratio distribution
    ax = axes[0, 2]
    if not df_all.empty:
        under_ratios = df_under['mean_log_ratio'].values if not df_under.empty else []
        over_ratios = df_over['mean_log_ratio'].values if not df_over.empty else []
        
        if len(under_ratios) > 0:
            ax.hist(under_ratios, bins=30, alpha=0.5, label='Under-attr', color='#ff7f0e')
        if len(over_ratios) > 0:
            ax.hist(over_ratios, bins=30, alpha=0.5, label='Over-attr', color='#2ca02c')
        
        ax.set_xlabel('Mean Log Ratio')
        ax.set_ylabel('Number of Features')
        ax.set_title('Log Ratio Distribution')
        ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)
        ax.legend()
    
    # 4. Class distribution
    ax = axes[1, 0]
    class_counts = defaultdict(lambda: {'under': 0, 'over': 0})
    
    for _, row in df_under.iterrows():
        class_counts[row['dominant_class']]['under'] += 1
    for _, row in df_over.iterrows():
        class_counts[row['dominant_class']]['over'] += 1
    
    if class_counts:
        classes = list(class_counts.keys())
        under_counts = [class_counts[c]['under'] for c in classes]
        over_counts = [class_counts[c]['over'] for c in classes]
        
        x = np.arange(len(classes))
        width = 0.35
        
        ax.bar(x - width/2, under_counts, width, label='Under-attr', color='#ff7f0e', alpha=0.7)
        ax.bar(x + width/2, over_counts, width, label='Over-attr', color='#2ca02c', alpha=0.7)
        
        ax.set_xlabel('Class')
        ax.set_ylabel('Number of Features')
        ax.set_title('Features by Dominant Class')
        ax.set_xticks(x)
        ax.set_xticklabels(classes, rotation=45, ha='right')
        ax.legend()
    
    # 5. Confidence score vs occurrences
    ax = axes[1, 1]
    if not df_all.empty and 'confidence_score' in df_all.columns:
        for category, df, color in [('Under-attr', df_under, '#ff7f0e'), 
                                     ('Over-attr', df_over, '#2ca02c')]:
            if not df.empty:
                ax.scatter(df['n_occurrences'], df['confidence_score'], 
                          alpha=0.5, label=category, color=color, s=20)
        
        ax.set_xlabel('Number of Occurrences')
        ax.set_ylabel('Confidence Score')
        ax.set_title('Confidence vs Occurrences')
        ax.set_xscale('log')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # 6. Top features comparison
    ax = axes[1, 2]
    n_top = 10
    
    if not df_under.empty:
        top_under = df_under.nlargest(n_top, 'confidence_score')
    else:
        top_under = pd.DataFrame()
    
    if not df_over.empty:
        top_over = df_over.nlargest(n_top, 'confidence_score')
    else:
        top_over = pd.DataFrame()
    
    y_pos = np.arange(n_top)
    
    if not top_under.empty:
        ax.barh(y_pos[:len(top_under)], top_under['confidence_score'].values, 
                height=0.35, alpha=0.7, color='#ff7f0e', label='Under-attr')
    
    if not top_over.empty:
        ax.barh(y_pos[:len(top_over)] + 0.4, top_over['confidence_score'].values, 
                height=0.35, alpha=0.7, color='#2ca02c', label='Over-attr')
    
    ax.set_xlabel('Confidence Score')
    ax.set_ylabel('Rank')
    ax.set_title(f'Top {n_top} Features by Confidence')
    ax.set_yticks(y_pos + 0.2)
    ax.set_yticklabels([f"#{i+1}" for i in range(n_top)])
    ax.legend()
    ax.invert_yaxis()
    
    plt.tight_layout()
    
    # Save figure
    save_file = save_path / f"feature_analysis_{Path(results_path).stem}.png"
    plt.savefig(save_file, dpi=150, bbox_inches='tight')
    print(f"\nVisualizations saved to: {save_file}")
    
    plt.show()
    
    return df_all


def main():
    """Main analysis function."""
    
    # Find all available result files
    result_files = list(Path('results').glob('saco_features_direct_l*.pt'))
    
    if not result_files:
        print("No feature analysis results found!")
        return
    
    print("Available feature analysis files:")
    for i, file in enumerate(result_files):
        print(f"  {i+1}. {file}")
    
    # Analyze the most recent or specified file
    for results_file in result_files:
        print(f"\n{'='*80}")
        print(f"Analyzing: {results_file}")
        print('='*80)
        
        # Generate comprehensive report
        print_analysis_report(str(results_file))
        
        # Create visualizations
        df_all = create_visualizations(str(results_file))
        
        # Save summary statistics to CSV
        summary_path = f"analysis_summary_{results_file.stem}.csv"
        if df_all is not None and not df_all.empty:
            df_all.to_csv(summary_path, index=False)
            print(f"\nSummary data saved to: {summary_path}")


if __name__ == "__main__":
    main()