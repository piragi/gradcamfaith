{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from vit_prisma.models.base_vit import HookedSAEViT\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "import torchvision # Required for the dataset with paths\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Vectorized Helper Functions ---\n",
    "\n",
    "def batch_gini(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates Gini coefficients for multiple features at once.\n",
    "    Args:\n",
    "        features: (n_patches, n_features) tensor\n",
    "    Returns:\n",
    "        (n_features,) tensor of Gini coefficients\n",
    "    \"\"\"\n",
    "    # Handle edge cases\n",
    "    if features.shape[1] == 0:\n",
    "        return torch.tensor([])\n",
    "    \n",
    "    # Sort each feature column\n",
    "    sorted_features, _ = torch.sort(features.abs(), dim=0)\n",
    "    n = features.shape[0]\n",
    "    cumsum = torch.cumsum(sorted_features, dim=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    cumsum_last = cumsum[-1].clamp(min=1e-8)\n",
    "    \n",
    "    return (n + 1 - 2 * cumsum.sum(dim=0) / cumsum_last) / n\n",
    "\n",
    "def batch_dice(features: torch.Tensor, target: torch.Tensor, threshold: float = 0.1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates Dice coefficients for multiple features against a target.\n",
    "    Args:\n",
    "        features: (n_patches, n_features) tensor\n",
    "        target: (n_patches,) tensor\n",
    "        threshold: threshold for binarization\n",
    "    Returns:\n",
    "        (n_features,) tensor of Dice coefficients\n",
    "    \"\"\"\n",
    "    feat_masks = (features > threshold).float()\n",
    "    target_mask = (target > threshold).float().unsqueeze(1)\n",
    "    \n",
    "    intersections = (feat_masks * target_mask).sum(dim=0)\n",
    "    unions = feat_masks.sum(dim=0) + target_mask.sum()\n",
    "    \n",
    "    return (2 * intersections / (unions + 1e-8))\n",
    "\n",
    "# --- Main Dictionary Building Function ---\n",
    "\n",
    "def build_attribution_aligned_feature_dictionary(\n",
    "    model: HookedSAEViT,\n",
    "    sae: SparseAutoencoder,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    attribution_dir: str,\n",
    "    n_samples: int = 1000,\n",
    "    layer_idx: int = 9,\n",
    "    patch_size: int = 16,\n",
    "    min_occurrences: int = 5,\n",
    "    save_path: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Builds a dictionary of features based on their alignment with baseline attribution maps,\n",
    "    using vectorized operations for efficiency.\n",
    "\n",
    "    Args:\n",
    "        model: Hooked SAE Vision Transformer.\n",
    "        sae: Sparse Autoencoder.\n",
    "        dataloader: A DataLoader that yields (image, label, path).\n",
    "        attribution_dir: Directory where pre-computed high-res attribution .npy files are stored.\n",
    "        n_samples: Maximum number of samples to process.\n",
    "        layer_idx: Transformer layer to analyze.\n",
    "        patch_size: The size of a single patch (e.g., 16 for ViT-B/16).\n",
    "        min_occurrences: Minimum times a feature must appear to be considered reliable.\n",
    "        save_path: Optional path to save the final dictionary.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing features and their alignment/localization metrics.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    feature_occurrences = defaultdict(list)\n",
    "    samples_processed = 0\n",
    "\n",
    "    resid_hook_name = f\"blocks.{layer_idx}.hook_resid_post\"\n",
    "    pbar = tqdm(dataloader, total=min(n_samples, len(dataloader)), desc=\"Analyzing feature alignment\")\n",
    "    reliable_features = {}\n",
    "\n",
    "    for imgs, labels, paths in pbar:\n",
    "        if samples_processed >= n_samples:\n",
    "            break\n",
    "\n",
    "        # Process one image at a time\n",
    "        image, label, path = imgs[0:1].to(device), labels[0].item(), paths[0]\n",
    "        \n",
    "        # --- 1. Get SAE feature activations ---\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(image, names_filter=[resid_hook_name])\n",
    "            resid = cache[resid_hook_name]\n",
    "            _, codes = sae.encode(resid)\n",
    "        \n",
    "        # Patch tokens only: (196, n_features)\n",
    "        feature_activations = codes[0, 1:]\n",
    "\n",
    "        # --- 2. Load and DOWNSAMPLE the corresponding attribution map ---\n",
    "        try:\n",
    "            # More robustly find the attribution file\n",
    "            img_filename_stem = Path(path).stem\n",
    "            parts = img_filename_stem.split('_')\n",
    "            prefix = 'train'\n",
    "            uuid = parts[0]\n",
    "            aug_part = parts[1]\n",
    "            \n",
    "            # Construct a glob pattern\n",
    "            attr_pattern = str(Path(attribution_dir) / f\"{prefix}_{uuid}_*_{aug_part}_attribution.npy\")\n",
    "            attr_files = glob.glob(attr_pattern)\n",
    "            \n",
    "            if not attr_files:\n",
    "                logging.warning(f\"Attribution not found for pattern {attr_pattern}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            attr_path = attr_files[0]\n",
    "            if len(attr_files) > 1:\n",
    "                logging.warning(f\"Multiple attributions for {img_filename_stem}, using first: {attr_path}\")\n",
    "\n",
    "            # Load and correctly downsample the map\n",
    "            attr_map_high_res = np.load(attr_path)\n",
    "            \n",
    "            # Ensure it's a 2D map before processing\n",
    "            if attr_map_high_res.ndim != 2:\n",
    "                logging.warning(f\"Unexpected attribution map shape {attr_map_high_res.shape} for {attr_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Reshape for pooling: (N, C, H, W) -> (1, 1, 224, 224)\n",
    "            attr_tensor_high_res = torch.from_numpy(attr_map_high_res).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "            # Use average pooling to downsample to patch resolution\n",
    "            attr_tensor_patch_level_2d = F.avg_pool2d(\n",
    "                attr_tensor_high_res,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size\n",
    "            )\n",
    "            \n",
    "            # Flatten to a vector for correlation: (1, 1, 14, 14) -> (196,)\n",
    "            attr_vec = attr_tensor_patch_level_2d.flatten()\n",
    "            \n",
    "            if attr_vec.shape[0] != feature_activations.shape[0]:\n",
    "                logging.warning(\n",
    "                    f\"Attribution map shape mismatch after downsampling for {img_filename_stem} \"\n",
    "                    f\"({attr_vec.shape[0]}) vs patch count \"\n",
    "                    f\"({feature_activations.shape[0]}). Check patch_size. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            # Normalize for stable dot products and correlation\n",
    "            attr_vec = (attr_vec - attr_vec.mean()) / (attr_vec.std() + 1e-8)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error loading or processing attribution for {img_filename_stem}: {e}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --- 3. VECTORIZED computation of alignment metrics for active features ---\n",
    "        active_feature_indices = (feature_activations.abs().sum(dim=0) > 1e-6).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(active_feature_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get all active features at once\n",
    "        active_features = feature_activations[:, active_feature_indices]  # (196, n_active)\n",
    "        \n",
    "        # 1. Vectorized correlations\n",
    "        # Normalize features\n",
    "        feat_means = active_features.mean(dim=0, keepdim=True)\n",
    "        feat_stds = active_features.std(dim=0, keepdim=True)\n",
    "        # Handle features with zero std\n",
    "        valid_std_mask = feat_stds.squeeze() > 1e-6\n",
    "        \n",
    "        if valid_std_mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Filter to only valid features\n",
    "        valid_indices = active_feature_indices[valid_std_mask]\n",
    "        valid_features = active_features[:, valid_std_mask]\n",
    "        valid_feat_means = feat_means[:, valid_std_mask]\n",
    "        valid_feat_stds = feat_stds[:, valid_std_mask]\n",
    "        \n",
    "        # Normalize\n",
    "        active_features_norm = (valid_features - valid_feat_means) / (valid_feat_stds + 1e-8)\n",
    "        \n",
    "        # Compute correlations using matrix multiplication\n",
    "        n_patches = len(attr_vec)\n",
    "        correlations = torch.matmul(attr_vec.unsqueeze(0), active_features_norm).squeeze() / (n_patches - 1)\n",
    "        \n",
    "        # 2. Vectorized AWA scores\n",
    "        awa_scores = torch.matmul(attr_vec.unsqueeze(0), valid_features).squeeze()\n",
    "        \n",
    "        # 3. Vectorized Gini coefficients\n",
    "        gini_scores = batch_gini(valid_features)\n",
    "        \n",
    "        # 4. Vectorized Dice scores\n",
    "        dice_scores = batch_dice(valid_features, attr_vec)\n",
    "        \n",
    "        # Store results\n",
    "        for i, feat_idx in enumerate(valid_indices):\n",
    "            feature_occurrences[feat_idx.item()].append({\n",
    "                'pfac_corr': correlations[i].item(),\n",
    "                'awa_score': awa_scores[i].item(),\n",
    "                'gini_score': gini_scores[i].item(),\n",
    "                'dice_score': dice_scores[i].item(),\n",
    "                'class': label\n",
    "            })\n",
    "\n",
    "        samples_processed += 1\n",
    "        pbar.set_postfix({\"Processed\": samples_processed, \"Active Features\": len(valid_indices)})\n",
    "    \n",
    "    # --- 4. Aggregate results and build final dictionary ---\n",
    "    for feat_id, occurrences in feature_occurrences.items():\n",
    "        if len(occurrences) < min_occurrences:\n",
    "            continue\n",
    "            \n",
    "        pfac_corrs = [o['pfac_corr'] for o in occurrences]\n",
    "        awa_scores = [o['awa_score'] for o in occurrences]\n",
    "        gini_scores = [o['gini_score'] for o in occurrences]\n",
    "        dice_scores = [o['dice_score'] for o in occurrences]\n",
    "        classes = [o['class'] for o in occurrences]\n",
    "        \n",
    "        mean_pfac = np.mean(pfac_corrs)\n",
    "        cv_pfac = np.std(pfac_corrs) / (abs(mean_pfac) + 1e-6)\n",
    "        consistency_score = mean_pfac * (1 - cv_pfac) * np.log1p(len(occurrences))\n",
    "        \n",
    "        reliable_features[feat_id] = {\n",
    "            'mean_pfac_corr': mean_pfac, \n",
    "            'std_pfac_corr': np.std(pfac_corrs), \n",
    "            'cv_pfac_corr': cv_pfac,\n",
    "            'mean_awa_score': np.mean(awa_scores), \n",
    "            'std_awa_score': np.std(awa_scores),\n",
    "            'mean_gini_score': np.mean(gini_scores), \n",
    "            'mean_dice_score': np.mean(dice_scores),\n",
    "            'consistency_score': consistency_score, \n",
    "            'occurrences': len(occurrences),\n",
    "            'classes_activated': list(set(classes)),\n",
    "            'raw_metrics': {\n",
    "                'pfac_corrs': pfac_corrs, \n",
    "                'awa_scores': awa_scores, \n",
    "                'gini_scores': gini_scores, \n",
    "                'dice_scores': dice_scores, \n",
    "                'classes': classes\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # --- 5. Finalize and Save Dictionary ---\n",
    "    if reliable_features:\n",
    "        sorted_features = sorted(reliable_features.items(), \n",
    "                               key=lambda item: item[1]['consistency_score'], \n",
    "                               reverse=True)\n",
    "        final_dict = {\n",
    "            'feature_stats': dict(sorted_features),\n",
    "            'metadata': {\n",
    "                'layer_idx': layer_idx, \n",
    "                'patch_size': patch_size, \n",
    "                'n_samples_processed': samples_processed, \n",
    "                'min_occurrences': min_occurrences, \n",
    "                'attribution_dir': attribution_dir\n",
    "            },\n",
    "            'metric_definitions': {\n",
    "                'pfac_corr': \"Pearson Correlation between feature activations and downsampled attribution map.\", \n",
    "                'awa_score': \"Attribution-Weighted Activation (dot product).\", \n",
    "                'gini_score': \"Spatial concentration of feature activations.\", \n",
    "                'dice_score': \"Spatial overlap (Dice) of thresholded maps.\", \n",
    "                'consistency_score': \"Overall score rewarding high, stable alignment and frequency.\"\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        final_dict = {'feature_stats': {}, 'metadata': {}}\n",
    "        logging.warning(\"No reliable features found meeting the criteria.\")\n",
    "\n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(final_dict, save_path)\n",
    "        logging.info(f\"Attribution alignment dictionary saved to {save_path}\")\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "924d71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from vit_prisma.models.base_vit import HookedSAEViT\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "import torchvision # Required for the dataset with paths\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Vectorized Helper Functions ---\n",
    "\n",
    "def batch_gini(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates Gini coefficients for multiple features at once.\n",
    "    Args:\n",
    "        features: (n_patches, n_features) tensor\n",
    "    Returns:\n",
    "        (n_features,) tensor of Gini coefficients\n",
    "    \"\"\"\n",
    "    # Handle edge cases\n",
    "    if features.shape[1] == 0:\n",
    "        return torch.tensor([])\n",
    "    \n",
    "    # Sort each feature column\n",
    "    sorted_features, _ = torch.sort(features.abs(), dim=0)\n",
    "    n = features.shape[0]\n",
    "    cumsum = torch.cumsum(sorted_features, dim=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    cumsum_last = cumsum[-1].clamp(min=1e-8)\n",
    "    \n",
    "    return (n + 1 - 2 * cumsum.sum(dim=0) / cumsum_last) / n\n",
    "\n",
    "def batch_dice(features: torch.Tensor, target: torch.Tensor, threshold: float = 0.1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates Dice coefficients for multiple features against a target.\n",
    "    Args:\n",
    "        features: (n_patches, n_features) tensor\n",
    "        target: (n_patches,) tensor\n",
    "        threshold: threshold for binarization\n",
    "    Returns:\n",
    "        (n_features,) tensor of Dice coefficients\n",
    "    \"\"\"\n",
    "    feat_masks = (features > threshold).float()\n",
    "    target_mask = (target > threshold).float().unsqueeze(1)\n",
    "    \n",
    "    intersections = (feat_masks * target_mask).sum(dim=0)\n",
    "    unions = feat_masks.sum(dim=0) + target_mask.sum()\n",
    "    \n",
    "    return (2 * intersections / (unions + 1e-8))\n",
    "\n",
    "def build_attribution_aligned_feature_dictionary(\n",
    "    model: HookedSAEViT,\n",
    "    sae: SparseAutoencoder,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    attribution_dir: str,\n",
    "    n_samples: int = 1000,\n",
    "    layer_idx: int = 9,\n",
    "    patch_size: int = 16,\n",
    "    min_occurrences: int = 5,\n",
    "    threshold: float = 0.10,\n",
    "    save_path: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    feature_occurrences = defaultdict(list)\n",
    "    samples_processed = 0\n",
    "\n",
    "    resid_hook_name = f\"blocks.{layer_idx}.hook_resid_post\"\n",
    "    pbar = tqdm(dataloader, total=min(n_samples, len(dataloader)),\n",
    "                desc=\"Analyzing feature alignment\")\n",
    "\n",
    "    for imgs, labels, paths in pbar:\n",
    "        if samples_processed >= n_samples:\n",
    "            break\n",
    "\n",
    "        img, label, path = imgs[0:1].to(device), labels[0].item(), paths[0]\n",
    "\n",
    "        # SAE codes\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(img, names_filter=[resid_hook_name])\n",
    "            resid = cache[resid_hook_name]\n",
    "            _, codes = sae.encode(resid)\n",
    "        feature_activations = codes[0, 1:]  # (n_patches, n_features)\n",
    "\n",
    "        # Attribution map\n",
    "        try:\n",
    "            stem    = Path(path).stem\n",
    "            parts   = stem.split('_')\n",
    "            prefix  = 'train'\n",
    "            uuid, aug = parts[0], parts[1]\n",
    "            pattern = str(Path(attribution_dir) / f\"{prefix}_{uuid}_*_{aug}_attribution.npy\")\n",
    "            attr_files = glob.glob(pattern)\n",
    "            if not attr_files:\n",
    "                logging.warning(f\"No attribution for pattern {pattern}\")\n",
    "                continue\n",
    "            attr_map = np.load(attr_files[0])\n",
    "            if attr_map.ndim != 2:\n",
    "                logging.warning(f\"Unexpected map shape {attr_map.shape} for {stem}\")\n",
    "                continue\n",
    "\n",
    "            attr_tensor = F.avg_pool2d(\n",
    "                torch.as_tensor(attr_map).float()\n",
    "                     .unsqueeze_(0).unsqueeze_(0).to(device),\n",
    "                kernel_size=patch_size, stride=patch_size\n",
    "            ).flatten()  # (n_patches,)\n",
    "\n",
    "            attr_tensor = (attr_tensor - attr_tensor.mean()) / (attr_tensor.std() + 1e-8)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Attribution load error for {stem}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Compute PFAC only\n",
    "        active_idx = (feature_activations.abs().sum(0) > 1e-6).nonzero(as_tuple=True)[0]\n",
    "        if active_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        feats = feature_activations[:, active_idx]  # (n_patches, n_active)\n",
    "        mu, sigma = feats.mean(0, keepdim=True), feats.std(0, keepdim=True)\n",
    "        valid = (sigma.squeeze() > 1e-6).nonzero(as_tuple=True)[0]\n",
    "        if valid.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        idx_valid = active_idx[valid]\n",
    "        feats_norm = (feats[:, valid] - mu[:, valid]) / (sigma[:, valid] + 1e-8)\n",
    "        n_patches = attr_tensor.numel()\n",
    "\n",
    "        pfac_vec = (attr_tensor @ feats_norm) / (n_patches - 1)  # (n_valid,)\n",
    "        for i, fid in enumerate(idx_valid):\n",
    "            feature_occurrences[fid.item()].append({\n",
    "                'pfac_corr': pfac_vec[i].item(),\n",
    "                'class':     label\n",
    "            })\n",
    "\n",
    "        samples_processed += 1\n",
    "        pbar.set_postfix({\"Processed\": samples_processed})\n",
    "\n",
    "    # Aggregate results\n",
    "    reliable_features: Dict[int, Any] = {}\n",
    "    for fid, occ in feature_occurrences.items():\n",
    "        if len(occ) < min_occurrences:\n",
    "            continue\n",
    "\n",
    "        pfacs   = [o['pfac_corr'] for o in occ]\n",
    "        classes = [o['class']      for o in occ]\n",
    "\n",
    "        mean_pfac = float(np.mean(pfacs))\n",
    "        class_count_map = Counter(classes)\n",
    "        class_mean_pfac = {}\n",
    "        for cls, cnt in class_count_map.items():\n",
    "            vals = [p for p,c in zip(pfacs, classes) if c == cls]\n",
    "            class_mean_pfac[cls] = float(np.mean(vals))\n",
    "\n",
    "        reliable_features[fid] = {\n",
    "            'mean_pfac_corr':  mean_pfac,\n",
    "            'occurrences':     len(occ),\n",
    "            'class_count_map': dict(class_count_map),\n",
    "            'class_mean_pfac': class_mean_pfac,\n",
    "            'raw_pfacs':       pfacs\n",
    "        }\n",
    "\n",
    "    final_dict = {\n",
    "        'feature_stats': reliable_features,\n",
    "        'metadata': {\n",
    "            'layer_idx': layer_idx,\n",
    "            'patch_size': patch_size,\n",
    "            'n_samples_processed': samples_processed,\n",
    "            'min_occurrences': min_occurrences,\n",
    "            'attribution_dir': attribution_dir,\n",
    "            'threshold': threshold\n",
    "        },\n",
    "        'metric_definitions': {\n",
    "            'pfac_corr': \"Pearson correlation between feature activations and attribution map\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(final_dict, save_path)\n",
    "        logging.info(f\"Dictionary saved to {save_path}\")\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "550b12d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:43:59 INFO:root: get_activation_fn received: activation_fn=topk, kwargs={'k': 1024}\n",
      "2025-07-09 17:43:59 WARNING:root: Model 'vit_base_patch16_224' is not in the lists of models passing or failing tests. Unclear status. You may want to check that the HookedViT matches the original model under tests/test_loading_clip.py.\n",
      "2025-07-09 17:44:00 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (39): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_pre not set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:44:01 INFO:timm.models._builder: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-07-09 17:44:01 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (40): huggingface.co:443\n",
      "2025-07-09 17:44:01 INFO:timm.models._hub: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-07-09 17:44:01 INFO:root: Filling in 2 missing keys with default initialization\n",
      "2025-07-09 17:44:01 WARNING:root: Missing key for weight matrix: head.W_H\n",
      "2025-07-09 17:44:01 INFO:root: Loaded pretrained model vit_base_patch16_224 into HookedTransformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the weights of a timm model to a Prisma ViT\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Converting the weights of a timm model to a Prisma ViT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing feature alignment: 100%|██████████| 65/65 [00:21<00:00,  3.04it/s, Processed=65]\n",
      "2025-07-09 17:44:24 INFO:root: Dictionary saved to ./sae_dictionaries/sfaf_stealth_l6_alignment_min10_32k512.pt\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from transmm_sfaf import get_processor_for_precached_224_images, IDX2CLS\n",
    "from vit_prisma.models.weight_conversion import convert_timm_weights\n",
    "import torch\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load SAE and fine-tuned model\"\"\"\n",
    "    # Load SAE\n",
    "    sae_path = \"./models/sweep/sae_k1024_exp32_lr2e-05/7c6cb9fc-vit_medical_sae_k_sweep/n_images_49276.pt\"\n",
    "    sae = SparseAutoencoder.load_from_pretrained(sae_path)\n",
    "    sae.cuda().eval()\n",
    "\n",
    "    # Load model\n",
    "    model = HookedSAEViT.from_pretrained(\"vit_base_patch16_224\")\n",
    "    model.head = torch.nn.Linear(model.cfg.d_model, 6)\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        \"./model/vit_b-ImageNet_class_init-frozen_False-dataset_Hyperkvasir_anatomical.pth\", weights_only=False\n",
    "    )\n",
    "    state_dict = checkpoint['model_state_dict'].copy()\n",
    "\n",
    "    if 'lin_head.weight' in state_dict:\n",
    "        state_dict['head.weight'] = state_dict.pop('lin_head.weight')\n",
    "    if 'lin_head.bias' in state_dict:\n",
    "        state_dict['head.bias'] = state_dict.pop('lin_head.bias')\n",
    "\n",
    "    converted_weights = convert_timm_weights(state_dict, model.cfg)\n",
    "    model.load_state_dict(converted_weights)\n",
    "    model.cuda().eval()\n",
    "\n",
    "    return sae, model\n",
    "\n",
    "sae, model = load_models()\n",
    "# Build new dictionary\n",
    "label_map = {2: 3, 3: 2}\n",
    "\n",
    "def custom_target_transform(target):\n",
    "    return label_map.get(target, target)\n",
    "\n",
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None: sample = self.transform(sample)\n",
    "        # Assuming you have a target_transform defined elsewhere\n",
    "        # if self.target_transform is not None: target = self.target_transform(target)\n",
    "        return sample, target, path\n",
    "\n",
    "train_dataset = ImageFolderWithPaths(\n",
    "    \"./hyper-kvasir_imagefolder/train\", \n",
    "    get_processor_for_precached_224_images()\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "layer_id = 6\n",
    "stealth_dict = build_attribution_aligned_feature_dictionary(\n",
    "    model, sae, dataloader, \n",
    "    n_samples=50000,\n",
    "    attribution_dir=\"./results/train/attributions\",\n",
    "    layer_idx=layer_id,\n",
    "    min_occurrences=10,           # Must appear 3+ times\n",
    "    save_path=f\"./sae_dictionaries/sfaf_stealth_l{layer_id}_alignment_min10_32k512.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f4c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 14:54:38 INFO:root: get_activation_fn received: activation_fn=topk, kwargs={'k': 1024}\n",
      "2025-07-09 14:54:38 WARNING:root: Model 'vit_base_patch16_224' is not in the lists of models passing or failing tests. Unclear status. You may want to check that the HookedViT matches the original model under tests/test_loading_clip.py.\n",
      "2025-07-09 14:54:38 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (17): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_pre not set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 14:54:39 INFO:timm.models._builder: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-07-09 14:54:39 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (18): huggingface.co:443\n",
      "2025-07-09 14:54:39 INFO:timm.models._hub: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-07-09 14:54:39 INFO:root: Filling in 2 missing keys with default initialization\n",
      "2025-07-09 14:54:39 WARNING:root: Missing key for weight matrix: head.W_H\n",
      "2025-07-09 14:54:39 INFO:root: Loaded pretrained model vit_base_patch16_224 into HookedTransformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the weights of a timm model to a Prisma ViT\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Converting the weights of a timm model to a Prisma ViT\n",
      "Loading existing S_f/A_f dictionary from ./sae_dictionaries/sfaf_stealth_l10_alignment_min10_32k512.pt\n",
      "Dictionary loaded successfully!\n",
      "Metadata: {'layer_idx': 10, 'patch_size': 16, 'n_samples_processed': 65, 'min_occurrences': 10, 'attribution_dir': './results/train/attributions'}\n",
      "--------------------------------------------------\n",
      "--- Top 20 Features by Mean PFAC (Alignment) ---\n",
      "      feature_id  mean_pfac_corr  std_pfac_corr  cv_pfac_corr  mean_awa_score  ...  mean_gini_score  mean_dice_score  \\\n",
      "1516       19927        0.113865       0.193980      1.703573        0.550749  ...         0.989824         0.041581   \n",
      "1684       21725        0.113816       0.202648      1.780465        1.010488  ...         0.968784         0.065548   \n",
      "1628       12579        0.113499       0.220535      1.943038        1.105776  ...         0.991201         0.054444   \n",
      "1486        8662        0.112895       0.199596      1.767974        0.245323  ...         0.989427         0.022286   \n",
      "2031       12609        0.110416       0.223012      2.019716        4.151282  ...         0.948381         0.115304   \n",
      "1543         305        0.110121       0.181233      1.645741        1.119808  ...         0.964056         0.099302   \n",
      "1632        2305        0.109612       0.193166      1.762263        0.795987  ...         0.921739         0.123744   \n",
      "2090       20230        0.108934       0.271763      2.494730        2.917290  ...         0.976057         0.104926   \n",
      "1515        7154        0.108552       0.198762      1.831007        0.079809  ...         0.993230         0.004706   \n",
      "2174       16705        0.108303       0.278465      2.571137        1.977661  ...         0.983369         0.093290   \n",
      "1840       22927        0.108293       0.198986      1.837449        3.002810  ...         0.865456         0.163324   \n",
      "2021       19381        0.106907       0.254113      2.376941        1.040330  ...         0.985887         0.074189   \n",
      "1615       16160        0.105350       0.203127      1.928100        0.640812  ...         0.985573         0.077577   \n",
      "2154       15793        0.105334       0.269284      2.556460        2.823756  ...         0.982217         0.109067   \n",
      "1749       23055        0.104654       0.198241      1.894234        0.172964  ...         0.989724         0.015464   \n",
      "1672       13589        0.104611       0.211850      2.025097        1.054687  ...         0.987854         0.071128   \n",
      "1838       13651        0.103246       0.231434      2.241553        1.188565  ...         0.987248         0.074877   \n",
      "1614        6632        0.101735       0.206270      2.027506        1.078934  ...         0.986218         0.091322   \n",
      "1574       23660        0.101195       0.196339      1.940188        0.608106  ...         0.987418         0.075199   \n",
      "1697         132        0.100224       0.210316      2.098431        0.961133  ...         0.986611         0.065128   \n",
      "2027       13673        0.099691       0.205925      2.065610        4.129117  ...         0.834463         0.188336   \n",
      "2200       13337        0.098699       0.261828      2.652763        1.640509  ...         0.968149         0.093444   \n",
      "1983       14617        0.097159       0.242260      2.493412        0.535508  ...         0.980237         0.026786   \n",
      "1763        7736        0.096902       0.221256      2.283265        0.816244  ...         0.988713         0.062865   \n",
      "2121       21686        0.096060       0.205124      2.135351        2.167991  ...         0.902990         0.163082   \n",
      "1437       11908        0.094426       0.147975      1.567086        2.587070  ...         0.796822         0.162220   \n",
      "1415        5839        0.093723       0.160134      1.708570        1.161519  ...         0.972275         0.070700   \n",
      "1336       24103        0.092862       0.148638      1.600615        0.250637  ...         0.991583         0.015143   \n",
      "2217        1887        0.092575       0.250476      2.705617        1.137429  ...         0.932100         0.102355   \n",
      "1702       16999        0.092569       0.206953      2.235642        0.052215  ...         0.989662         0.013732   \n",
      "1986       13696        0.092372       0.218528      2.365717        1.407620  ...         0.981030         0.085045   \n",
      "1801       23390        0.092300       0.189701      2.055254        0.489073  ...         0.976818         0.045234   \n",
      "1691       10723        0.092111       0.178851      1.941661        0.579525  ...         0.981961         0.057045   \n",
      "1806       15756        0.092109       0.208553      2.264168        1.342918  ...         0.975113         0.092665   \n",
      "2158       11100        0.091859       0.250827      2.730543        2.444783  ...         0.944537         0.124741   \n",
      "1818        1530        0.091802       0.220581      2.402774        0.145018  ...         0.992126         0.018006   \n",
      "2140       13765        0.091604       0.252817      2.759875        1.680545  ...         0.985148         0.085808   \n",
      "1737        4465        0.090633       0.200006      2.206734        0.865315  ...         0.984962         0.086372   \n",
      "1964       23269        0.089819       0.206925      2.303780        0.964439  ...         0.984392         0.061378   \n",
      "1484       19142        0.089651       0.175755      1.960404        1.019659  ...         0.980207         0.056752   \n",
      "1811        6453        0.089032       0.213238      2.395042        0.442364  ...         0.991442         0.009848   \n",
      "2066        6702        0.088830       0.250599      2.821072        1.392950  ...         0.986958         0.060888   \n",
      "1273        7915        0.088692       0.107945      1.217061        2.401071  ...         0.809521         0.243457   \n",
      "1621        9557        0.088025       0.190155      2.160221        0.694362  ...         0.992943         0.038240   \n",
      "1366        1546        0.088011       0.150403      1.708884        0.907136  ...         0.968858         0.069820   \n",
      "2032       17331        0.086616       0.243078      2.806368        1.871905  ...         0.982937         0.090576   \n",
      "1857        3226        0.086352       0.226983      2.628541        0.413754  ...         0.983421         0.053235   \n",
      "1492        3785        0.086341       0.165771      1.919933        1.300351  ...         0.972857         0.087295   \n",
      "2082       11520        0.086217       0.248106      2.877675        0.805882  ...         0.989822         0.052609   \n",
      "1794       13129        0.085829       0.203763      2.374024        0.754461  ...         0.989091         0.061139   \n",
      "\n",
      "      consistency_score  occurrences  classes_activated  \n",
      "1516          -0.216949           14                [2]  \n",
      "1684          -0.270444           20       [2, 3, 4, 5]  \n",
      "1628          -0.256657           10                [2]  \n",
      "1486          -0.207898           10                [2]  \n",
      "2031          -0.412493           38    [0, 2, 3, 4, 5]  \n",
      "1543          -0.225991           23             [2, 5]  \n",
      "1632          -0.258265           21          [2, 4, 5]  \n",
      "2090          -0.440944           14                [2]  \n",
      "1515          -0.216309           10             [2, 5]  \n",
      "2174          -0.491824           17                [2]  \n",
      "1840          -0.329894           37    [0, 2, 3, 4, 5]  \n",
      "2021          -0.408137           15                [2]  \n",
      "1615          -0.250789           12             [2, 5]  \n",
      "2154          -0.473871           17                [2]  \n",
      "1749          -0.293435           22    [0, 2, 3, 4, 5]  \n",
      "1672          -0.266473           11                [2]  \n",
      "1838          -0.328790           12                [2]  \n",
      "1614          -0.250659           10                [2]  \n",
      "1574          -0.236419           11                [2]  \n",
      "1697          -0.273562           11                [2]  \n",
      "2027          -0.411245           47    [0, 2, 3, 4, 5]  \n",
      "2200          -0.511482           22          [2, 4, 5]  \n",
      "1983          -0.392935           14             [0, 4]  \n",
      "1763          -0.298182           10                [2]  \n",
      "2121          -0.456932           65    [0, 2, 3, 4, 5]  \n",
      "1437          -0.190380           34          [2, 4, 5]  \n",
      "1415          -0.179840           14          [2, 4, 5]  \n",
      "1336          -0.133741           10          [2, 4, 5]  \n",
      "2217          -0.547232           31       [0, 2, 4, 5]  \n",
      "1702          -0.274276           10             [2, 4]  \n",
      "1986          -0.395554           22             [2, 5]  \n",
      "1801          -0.313517           24       [2, 3, 4, 5]  \n",
      "1691          -0.271965           22       [0, 2, 3, 4]  \n",
      "1806          -0.315329           14             [2, 4]  \n",
      "2158          -0.476218           19                [2]  \n",
      "1818          -0.319999           11             [2, 5]  \n",
      "2140          -0.465959           17                [2]  \n",
      "1737          -0.288635           13                [2]  \n",
      "1964          -0.385955           26          [2, 4, 5]  \n",
      "1484          -0.206462           10             [2, 5]  \n",
      "1811          -0.318576           12          [2, 4, 5]  \n",
      "2066          -0.426910           13                [2]  \n",
      "1273          -0.050806           13          [2, 4, 5]  \n",
      "1621          -0.253778           11             [2, 5]  \n",
      "1366          -0.149604           10             [2, 5]  \n",
      "2032          -0.412907           13                [2]  \n",
      "1857          -0.337211           10                [2]  \n",
      "1492          -0.209615           13             [2, 5]  \n",
      "2082          -0.438398           14                [2]  \n",
      "1794          -0.311228           13                [2]  \n",
      "\n",
      "[50 rows x 11 columns]\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transmm_sfaf import load_models, get_processor_for_precached_224_images, IDX2CLS\n",
    "from transmm_sfaf import load_or_build_sf_af_dictionary\n",
    "\n",
    "sae, model = load_models()\n",
    "layer_idx = 10\n",
    "stealth_dict = load_or_build_sf_af_dictionary(\n",
    "    model,\n",
    "    sae,\n",
    "    n_samples=50000,\n",
    "    layer_idx=layer_idx,\n",
    "    dict_path=f\"./sae_dictionaries/sfaf_stealth_l{layer_idx}_alignment_min10_32k512.pt\",\n",
    "    rebuild=False\n",
    ")\n",
    "\n",
    "print(\"Dictionary loaded successfully!\")\n",
    "print(\"Metadata:\", stealth_dict['metadata'])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Convert to a Pandas DataFrame for easy analysis ---\n",
    "feature_stats = stealth_dict.get('feature_stats', {})\n",
    "if not feature_stats:\n",
    "    print(\"No feature statistics found in the dictionary.\")\n",
    "else:\n",
    "    # We convert the dictionary of features into a list of dictionaries, then to a DataFrame\n",
    "    feature_list = []\n",
    "    for feat_id, stats in feature_stats.items():\n",
    "        # Create a flat dictionary for the DataFrame row\n",
    "        row = {'feature_id': feat_id}\n",
    "        # Add all stats except the bulky raw_metrics\n",
    "        row.update({k: v for k, v in stats.items() if k != 'raw_metrics'})\n",
    "        feature_list.append(row)\n",
    "\n",
    "    df = pd.DataFrame(feature_list)\n",
    "\n",
    "    # Set display options for better readability\n",
    "    pd.set_option('display.max_rows', 50)\n",
    "    pd.set_option('display.width', 120)\n",
    "    \n",
    "    print(\"--- Top 20 Features by Mean PFAC (Alignment) ---\")\n",
    "    # PFAC = Patch-Feature Attribution Correlation\n",
    "    print(df.sort_values(by='mean_pfac_corr', ascending=False).head(100).tail(50))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea5c981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "      TOP 20 CORRELATED FEATURES PER CLASS (ranked by avg_pfac_corr) for Layer 10\n",
      "      (Minimum Occurrences per Class: 10)\n",
      "================================================================================\n",
      "PFAC = Patch-Feature Attribution Correlation\n",
      "\n",
      "                           class_name  avg_pfac_corr  occurrences\n",
      "class_id feature_id                                              \n",
      "5        12609                 z-line       0.340186           11\n",
      "         13673                 z-line       0.336493           11\n",
      "         14977                 z-line       0.285648           10\n",
      "         12153                 z-line       0.270660           11\n",
      "         10860                 z-line       0.270145           11\n",
      "         5286                  z-line       0.270063           10\n",
      "         465                   z-line       0.268521           11\n",
      "         8735                  z-line       0.265743           10\n",
      "         2605                  z-line       0.261874           11\n",
      "         14400                 z-line       0.258170           11\n",
      "         2203                  z-line       0.253668           11\n",
      "         10315                 z-line       0.240885           11\n",
      "         15498                 z-line       0.238697           11\n",
      "         22110                 z-line       0.221261           11\n",
      "         5858                  z-line       0.220652           11\n",
      "         17519                 z-line       0.219016           11\n",
      "         2073                  z-line       0.218902           10\n",
      "         16071                 z-line       0.218677           11\n",
      "         12320                 z-line       0.216653           11\n",
      "         11633                 z-line       0.208127           11\n",
      "2        418         retroflex-rectum       0.159776           10\n",
      "         24146       retroflex-rectum       0.158485           12\n",
      "         20365       retroflex-rectum       0.153805           12\n",
      "         6214        retroflex-rectum       0.153664           13\n",
      "         7675        retroflex-rectum       0.152340           11\n",
      "         10067       retroflex-rectum       0.145220           12\n",
      "         7829        retroflex-rectum       0.141076           10\n",
      "         4699        retroflex-rectum       0.138549           12\n",
      "         9909        retroflex-rectum       0.136109           10\n",
      "         2012        retroflex-rectum       0.135559           11\n",
      "0        12311                  cecum       0.135364           11\n",
      "2        17943       retroflex-rectum       0.134835           13\n",
      "         8321        retroflex-rectum       0.132035           13\n",
      "         21686       retroflex-rectum       0.129880           22\n",
      "         12403       retroflex-rectum       0.127418           10\n",
      "         4645        retroflex-rectum       0.126142           14\n",
      "         10405       retroflex-rectum       0.123743           14\n",
      "         2827        retroflex-rectum       0.122954           11\n",
      "         12540       retroflex-rectum       0.120600           11\n",
      "         12892       retroflex-rectum       0.120467           15\n",
      "         536         retroflex-rectum       0.120285           12\n",
      "0        9986                   cecum       0.103188           21\n",
      "         1082                   cecum       0.096051           21\n",
      "         13644                  cecum       0.092963           21\n",
      "         14286                  cecum       0.091915           10\n",
      "         20713                  cecum       0.089828           21\n",
      "         17383                  cecum       0.084935           21\n",
      "         2136                   cecum       0.084453           10\n",
      "         20514                  cecum       0.082800           14\n",
      "         19397                  cecum       0.081987           21\n",
      "         10233                  cecum       0.081252           10\n",
      "         10776                  cecum       0.080104           21\n",
      "         24480                  cecum       0.079687           16\n",
      "         3281                   cecum       0.079005           20\n",
      "         13289                  cecum       0.076888           21\n",
      "         15970                  cecum       0.075885           21\n",
      "         3448                   cecum       0.075645           12\n",
      "         5414                   cecum       0.074755           13\n",
      "         2333                   cecum       0.074449           14\n",
      "         14327                  cecum       0.074136           21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# Assuming these are already loaded from your setup code\n",
    "# from transmm_sfaf import load_models, IDX2CLS, load_or_build_sf_af_dictionary\n",
    "# sae, model = load_models()\n",
    "# layer_idx = 7\n",
    "# stealth_dict = load_or_build_sf_af_dictionary(...)\n",
    "# print(\"Dictionary loaded successfully!\")\n",
    "\n",
    "# --- Configuration ---\n",
    "TOP_K_PER_CLASS = 20\n",
    "# Add a minimum occurrence filter to ensure statistical significance\n",
    "MIN_OCCURRENCES_FOR_CLASS = 10\n",
    "\n",
    "# --- 1. Process Raw Data into a Per-Class Structure ---\n",
    "\n",
    "feature_stats = stealth_dict.get('feature_stats', {})\n",
    "if not feature_stats:\n",
    "    print(\"No feature statistics found in the dictionary.\")\n",
    "else:\n",
    "    per_class_feature_data = []\n",
    "\n",
    "    # Iterate through each feature in the dictionary\n",
    "    for feat_id, stats in feature_stats.items():\n",
    "        raw_metrics = stats.get('raw_metrics', {})\n",
    "        pfac_corrs = raw_metrics.get('pfac_corrs', [])\n",
    "        classes = raw_metrics.get('classes', [])\n",
    "\n",
    "        if not pfac_corrs or not classes:\n",
    "            continue\n",
    "\n",
    "        # Group correlations by class for the current feature\n",
    "        corrs_by_class = {}\n",
    "        for corr, cls_id in zip(pfac_corrs, classes):\n",
    "            if cls_id not in corrs_by_class:\n",
    "                corrs_by_class[cls_id] = []\n",
    "            corrs_by_class[cls_id].append(corr)\n",
    "\n",
    "        # Calculate stats for each class this feature appeared in\n",
    "        for cls_id, corrs_list in corrs_by_class.items():\n",
    "            valid_corrs = [c for c in corrs_list if not np.isnan(c)]\n",
    "            occurrences = len(valid_corrs)\n",
    "\n",
    "            # --- NEW: Filter based on minimum occurrences ---\n",
    "            if occurrences < MIN_OCCURRENCES_FOR_CLASS:\n",
    "                continue  # Skip if this feature-class pair is too rare\n",
    "\n",
    "            avg_corr = np.mean(valid_corrs)\n",
    "            \n",
    "            per_class_feature_data.append({\n",
    "                'feature_id': feat_id,\n",
    "                'class_id': cls_id,\n",
    "                'class_name': IDX2CLS.get(cls_id, 'Unknown'), # Still useful for display\n",
    "                'avg_pfac_corr': avg_corr, # The key metric for ranking\n",
    "                'occurrences': occurrences, # How often it activated for this class\n",
    "            })\n",
    "\n",
    "    # --- 2. Create the DataFrame and Perform Analysis ---\n",
    "    \n",
    "    if not per_class_feature_data:\n",
    "        print(f\"No valid per-class feature data could be extracted with min occurrences >= {MIN_OCCURRENCES_FOR_CLASS}.\")\n",
    "    else:\n",
    "        df_per_class = pd.DataFrame(per_class_feature_data)\n",
    "\n",
    "        # Set display options for better readability\n",
    "        pd.set_option('display.max_rows', 500)\n",
    "        pd.set_option('display.max_columns', 10)\n",
    "        pd.set_option('display.width', 150)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"      TOP {TOP_K_PER_CLASS} CORRELATED FEATURES PER CLASS (ranked by avg_pfac_corr) for Layer {layer_idx}\")\n",
    "        print(f\"      (Minimum Occurrences per Class: {MIN_OCCURRENCES_FOR_CLASS})\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"PFAC = Patch-Feature Attribution Correlation\\n\")\n",
    "\n",
    "        # --- 3. Group by Class ID and Display Results ---\n",
    "        \n",
    "        # Sort by correlation, then group by class ID and take the top N\n",
    "        top_features_per_class = (\n",
    "            df_per_class\n",
    "            .sort_values(by='avg_pfac_corr', ascending=False)\n",
    "            .groupby('class_id', sort=False) # Group by class_id instead of class_name\n",
    "            .head(TOP_K_PER_CLASS)\n",
    "        )\n",
    "        \n",
    "        # Set the multi-index using class_id and feature_id for clear grouping\n",
    "        top_features_per_class = top_features_per_class.set_index(['class_id', 'feature_id'])\n",
    "\n",
    "        print(top_features_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98d2de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- Statistics for Specific Features: [8, 9, 12, 23, 26] ---\n",
      "feature_id                      12                  23        9             8                   26\n",
      "mean_pfac_corr            -0.01718           -0.018236  0.073728      0.006976            0.005308\n",
      "std_pfac_corr             0.085807            0.071139  0.162014      0.086522            0.094934\n",
      "cv_pfac_corr              4.994277            3.900749  2.197426     12.400353           17.882137\n",
      "mean_awa_score           -1.019369           -0.569488  1.565944      0.211732            0.099672\n",
      "std_awa_score             3.391973            2.094591  3.699006      2.186467            1.626569\n",
      "mean_gini_score           0.967501             0.97359  0.990795      0.985377            0.991927\n",
      "mean_dice_score           0.043068            0.040191  0.046066      0.034196            0.016611\n",
      "consistency_score         0.561156            0.429977 -0.498712     -0.534286           -0.647333\n",
      "occurrences                   3559                3388       283           826                1371\n",
      "classes_activated  [1, 2, 3, 4, 5]  [0, 1, 2, 3, 4, 5]    [2, 5]  [2, 3, 4, 5]  [0, 1, 2, 3, 4, 5]\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the list of feature IDs you want to inspect\n",
    "specific_feature_ids = [8, 9, 12, 23, 26]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"--- Statistics for Specific Features: {specific_feature_ids} ---\")\n",
    "\n",
    "# 2. Filter the DataFrame to get only the rows for these features\n",
    "# .isin() checks if the value in the 'feature_id' column is present in your list\n",
    "specific_features_df = df[df['feature_id'].isin(specific_feature_ids)]\n",
    "\n",
    "# 3. Print the resulting filtered DataFrame\n",
    "if not specific_features_df.empty:\n",
    "    # We can transpose (.T) the DataFrame for better readability when there are few features\n",
    "    # This turns columns into rows, which is often easier to read.\n",
    "    print(specific_features_df.set_index('feature_id').T)\n",
    "else:\n",
    "    print(\"None of the specified features were found in the dictionary.\")\n",
    "    print(\"This might be because they didn't meet the min_occurrences threshold.\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31ebfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 14:47:40 INFO:root: get_activation_fn received: activation_fn=topk, kwargs={'k': 128}\n",
      "2025-07-07 14:47:40 WARNING:root: Model 'vit_base_patch16_224' is not in the lists of models passing or failing tests. Unclear status. You may want to check that the HookedViT matches the original model under tests/test_loading_clip.py.\n",
      "2025-07-07 14:47:41 DEBUG:urllib3.connectionpool: Resetting dropped connection: huggingface.co\n",
      "2025-07-07 14:47:41 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "2025-07-07 14:47:41 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /api/resolve-cache/models/timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/063c6c38a5d8510b2e57df480445e94b231dad2c/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_pre not set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 14:47:42 INFO:timm.models._builder: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-07-07 14:47:42 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/model.safetensors HTTP/1.1\" 302 0\n",
      "2025-07-07 14:47:42 INFO:timm.models._hub: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-07-07 14:47:42 INFO:root: Filling in 2 missing keys with default initialization\n",
      "2025-07-07 14:47:42 WARNING:root: Missing key for weight matrix: head.W_H\n",
      "2025-07-07 14:47:42 INFO:root: Loaded pretrained model vit_base_patch16_224 into HookedTransformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the weights of a timm model to a Prisma ViT\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Converting the weights of a timm model to a Prisma ViT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building comprehensive dict: 100%|██████████| 750/750 [01:39<00:00,  7.57it/s, Processed=750, Features Found=936]\n",
      "2025-07-07 14:49:22 INFO:root: Comprehensive dictionary saved to ./sae_dictionaries/sfaf_stealth_l9_alignment_comp.pt\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from transmm_sfaf import load_models, get_processor_for_precached_224_images, IDX2CLS\n",
    "import torch\n",
    "\n",
    "\n",
    "sae, model = load_models()\n",
    "# Build new dictionary\n",
    "label_map = {2: 3, 3: 2}\n",
    "\n",
    "def custom_target_transform(target):\n",
    "    return label_map.get(target, target)\n",
    "\n",
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None: sample = self.transform(sample)\n",
    "        # Assuming you have a target_transform defined elsewhere\n",
    "        # if self.target_transform is not None: target = self.target_transform(target)\n",
    "        return sample, target, path\n",
    "\n",
    "train_dataset = ImageFolderWithPaths(\n",
    "    \"./hyper-kvasir_imagefolder/dev\", \n",
    "    get_processor_for_precached_224_images()\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "layer_id = 9\n",
    "stealth_dict = build_comprehensive_feature_dictionary(\n",
    "    model, sae, dataloader, \n",
    "    n_samples=50000,\n",
    "    attribution_dir=\"./results/dev/attributions\",\n",
    "    layer_idx=layer_id,\n",
    "    min_occurrences=3,           # Must appear 3+ times\n",
    "    save_path=f\"./sae_dictionaries/sfaf_stealth_l{layer_id}_alignment_comp.pt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradcamfaith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
