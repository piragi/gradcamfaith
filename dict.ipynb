{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924d71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from vit_prisma.models.base_vit import HookedSAEViT\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "import torchvision # Required for the dataset with paths\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Vectorized Helper Functions ---\n",
    "\n",
    "def batch_gini(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates Gini coefficients for multiple features at once.\n",
    "    Args:\n",
    "        features: (n_patches, n_features) tensor\n",
    "    Returns:\n",
    "        (n_features,) tensor of Gini coefficients\n",
    "    \"\"\"\n",
    "    # Handle edge cases\n",
    "    if features.shape[1] == 0:\n",
    "        return torch.tensor([])\n",
    "    \n",
    "    # Sort each feature column\n",
    "    sorted_features, _ = torch.sort(features.abs(), dim=0)\n",
    "    n = features.shape[0]\n",
    "    cumsum = torch.cumsum(sorted_features, dim=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    cumsum_last = cumsum[-1].clamp(min=1e-8)\n",
    "    \n",
    "    return (n + 1 - 2 * cumsum.sum(dim=0) / cumsum_last) / n\n",
    "\n",
    "def batch_dice(features: torch.Tensor, target: torch.Tensor, threshold: float = 0.1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates Dice coefficients for multiple features against a target.\n",
    "    Args:\n",
    "        features: (n_patches, n_features) tensor\n",
    "        target: (n_patches,) tensor\n",
    "        threshold: threshold for binarization\n",
    "    Returns:\n",
    "        (n_features,) tensor of Dice coefficients\n",
    "    \"\"\"\n",
    "    feat_masks = (features > threshold).float()\n",
    "    target_mask = (target > threshold).float().unsqueeze(1)\n",
    "    \n",
    "    intersections = (feat_masks * target_mask).sum(dim=0)\n",
    "    unions = feat_masks.sum(dim=0) + target_mask.sum()\n",
    "    \n",
    "    return (2 * intersections / (unions + 1e-8))\n",
    "\n",
    "def build_attribution_aligned_feature_dictionary(\n",
    "    model: HookedSAEViT,\n",
    "    sae: SparseAutoencoder,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    attribution_dir: str,\n",
    "    n_samples: int = 1000,\n",
    "    layer_idx: int = 9,\n",
    "    patch_size: int = 16,\n",
    "    min_occurrences: int = 5,\n",
    "    threshold: float = 0.10,\n",
    "    save_path: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    feature_occurrences = defaultdict(list)\n",
    "    samples_processed = 0\n",
    "\n",
    "    resid_hook_name = f\"blocks.{layer_idx}.hook_resid_post\"\n",
    "    pbar = tqdm(dataloader, total=min(n_samples, len(dataloader)),\n",
    "                desc=\"Analyzing feature alignment\")\n",
    "\n",
    "    for imgs, labels, paths in pbar:\n",
    "        if samples_processed >= n_samples:\n",
    "            break\n",
    "\n",
    "        img, label, path = imgs[0:1].to(device), labels[0].item(), paths[0]\n",
    "\n",
    "        # SAE codes\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(img, names_filter=[resid_hook_name])\n",
    "            resid = cache[resid_hook_name]\n",
    "            _, codes = sae.encode(resid)\n",
    "        feature_activations = codes[0, 1:]  # (n_patches, n_features)\n",
    "\n",
    "        # Attribution map\n",
    "        try:\n",
    "            stem    = Path(path).stem\n",
    "            parts   = stem.split('_')\n",
    "            prefix  = 'train'\n",
    "            uuid, aug = parts[0], parts[1]\n",
    "            pattern = str(Path(attribution_dir) / f\"{prefix}_{uuid}_*_{aug}_attribution.npy\")\n",
    "            attr_files = glob.glob(pattern)\n",
    "            if not attr_files:\n",
    "                logging.warning(f\"No attribution for pattern {pattern}\")\n",
    "                continue\n",
    "            attr_map = np.load(attr_files[0])\n",
    "            if attr_map.ndim != 2:\n",
    "                logging.warning(f\"Unexpected map shape {attr_map.shape} for {stem}\")\n",
    "                continue\n",
    "\n",
    "            attr_tensor = F.avg_pool2d(\n",
    "                torch.as_tensor(attr_map).float()\n",
    "                     .unsqueeze_(0).unsqueeze_(0).to(device),\n",
    "                kernel_size=patch_size, stride=patch_size\n",
    "            ).flatten()  # (n_patches,)\n",
    "\n",
    "            attr_tensor = (attr_tensor - attr_tensor.mean()) / (attr_tensor.std() + 1e-8)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Attribution load error for {stem}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Compute PFAC only\n",
    "        active_idx = (feature_activations.abs().sum(0) > 1e-6).nonzero(as_tuple=True)[0]\n",
    "        if active_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        feats = feature_activations[:, active_idx]  # (n_patches, n_active)\n",
    "        mu, sigma = feats.mean(0, keepdim=True), feats.std(0, keepdim=True)\n",
    "        valid = (sigma.squeeze() > 1e-6).nonzero(as_tuple=True)[0]\n",
    "        if valid.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        idx_valid = active_idx[valid]\n",
    "        feats_norm = (feats[:, valid] - mu[:, valid]) / (sigma[:, valid] + 1e-8)\n",
    "        n_patches = attr_tensor.numel()\n",
    "\n",
    "        pfac_vec = (attr_tensor @ feats_norm) / (n_patches - 1)  # (n_valid,)\n",
    "        for i, fid in enumerate(idx_valid):\n",
    "            feature_occurrences[fid.item()].append({\n",
    "                'pfac_corr': pfac_vec[i].item(),\n",
    "                'class':     label\n",
    "            })\n",
    "\n",
    "        samples_processed += 1\n",
    "        pbar.set_postfix({\"Processed\": samples_processed})\n",
    "\n",
    "    # Aggregate results\n",
    "    reliable_features: Dict[int, Any] = {}\n",
    "    for fid, occ in feature_occurrences.items():\n",
    "        if len(occ) < min_occurrences:\n",
    "            continue\n",
    "\n",
    "        pfacs   = [o['pfac_corr'] for o in occ]\n",
    "        classes = [o['class']      for o in occ]\n",
    "\n",
    "        mean_pfac = float(np.mean(pfacs))\n",
    "        class_count_map = Counter(classes)\n",
    "        class_mean_pfac = {}\n",
    "        for cls, cnt in class_count_map.items():\n",
    "            vals = [p for p,c in zip(pfacs, classes) if c == cls]\n",
    "            class_mean_pfac[cls] = float(np.mean(vals))\n",
    "\n",
    "        reliable_features[fid] = {\n",
    "            'mean_pfac_corr':  mean_pfac,\n",
    "            'occurrences':     len(occ),\n",
    "            'class_count_map': dict(class_count_map),\n",
    "            'class_mean_pfac': class_mean_pfac,\n",
    "            'raw_pfacs':       pfacs\n",
    "        }\n",
    "\n",
    "    final_dict = {\n",
    "        'feature_stats': reliable_features,\n",
    "        'metadata': {\n",
    "            'layer_idx': layer_idx,\n",
    "            'patch_size': patch_size,\n",
    "            'n_samples_processed': samples_processed,\n",
    "            'min_occurrences': min_occurrences,\n",
    "            'attribution_dir': attribution_dir,\n",
    "            'threshold': threshold\n",
    "        },\n",
    "        'metric_definitions': {\n",
    "            'pfac_corr': \"Pearson correlation between feature activations and attribution map\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(final_dict, save_path)\n",
    "        logging.info(f\"Dictionary saved to {save_path}\")\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b12d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 14:17:59 INFO:root: get_activation_fn received: activation_fn=topk, kwargs={'k': 128}\n",
      "2025-07-13 14:17:59 WARNING:root: Model 'vit_base_patch16_224' is not in the lists of models passing or failing tests. Unclear status. You may want to check that the HookedViT matches the original model under tests/test_loading_clip.py.\n",
      "2025-07-13 14:18:00 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-07-13 14:18:00 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "2025-07-13 14:18:00 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /api/resolve-cache/models/timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/063c6c38a5d8510b2e57df480445e94b231dad2c/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_pre not set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 14:18:01 INFO:timm.models._builder: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-07-13 14:18:01 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/model.safetensors HTTP/1.1\" 302 0\n",
      "2025-07-13 14:18:01 INFO:timm.models._hub: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-07-13 14:18:01 INFO:root: Filling in 2 missing keys with default initialization\n",
      "2025-07-13 14:18:01 WARNING:root: Missing key for weight matrix: head.W_H\n",
      "2025-07-13 14:18:01 INFO:root: Loaded pretrained model vit_base_patch16_224 into HookedTransformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the weights of a timm model to a Prisma ViT\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Converting the weights of a timm model to a Prisma ViT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing feature alignment:   5%|▍         | 3/65 [00:00<00:17,  3.48it/s, Processed=3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m layer_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m---> 55\u001b[0m stealth_dict \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_attribution_aligned_feature_dictionary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribution_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/train/attributions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_occurrences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Must appear 3+ times\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./sae_dictionaries/sfaf_stealth_l\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_alignment_min1_32k32.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     62\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 104\u001b[0m, in \u001b[0;36mbuild_attribution_aligned_feature_dictionary\u001b[0;34m(model, sae, dataloader, attribution_dir, n_samples, layer_idx, patch_size, min_occurrences, threshold, save_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m uuid, aug \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m0\u001b[39m], parts[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    103\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(Path(attribution_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maug\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_attribution.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m attr_files \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr_files:\n\u001b[1;32m    106\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo attribution for pattern \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/glob.py:24\u001b[0m, in \u001b[0;36mglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mglob\u001b[39m(pathname, \u001b[38;5;241m*\u001b[39m, root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dir_fd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of paths matching a pathname pattern.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    The pattern may contain simple shell-style wildcards a la\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    zero or more directories and subdirectories.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/glob.py:86\u001b[0m, in \u001b[0;36m_iglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, dironly)\u001b[0m\n\u001b[1;32m     84\u001b[0m     glob_in_dir \u001b[38;5;241m=\u001b[39m _glob0\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirname \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, name)\n",
      "File \u001b[0;32m/usr/lib/python3.10/glob.py:94\u001b[0m, in \u001b[0;36m_glob1\u001b[0;34m(dirname, pattern, dir_fd, dironly)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_glob1\u001b[39m(dirname, pattern, dir_fd, dironly):\n\u001b[0;32m---> 94\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[43m_listdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(pattern):\n\u001b[1;32m     96\u001b[0m         names \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(x))\n",
      "File \u001b[0;32m/usr/lib/python3.10/glob.py:164\u001b[0m, in \u001b[0;36m_listdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_listdir\u001b[39m(dirname, dir_fd, dironly):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(_iterdir(dirname, dir_fd, dironly)) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/glob.py:149\u001b[0m, in \u001b[0;36m_iterdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dironly \u001b[38;5;129;01mor\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m fsencode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m fsencode(entry\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from transmm_sfaf import get_processor_for_precached_224_images, IDX2CLS\n",
    "from vit_prisma.models.weight_conversion import convert_timm_weights\n",
    "import torch\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load SAE and fine-tuned model\"\"\"\n",
    "    # Load SAE\n",
    "    sae_path = \"./models/sweep/sae_l7_k128_exp32_lr2e-05/d77c1ce8-vit_medical_sae_k_sweep/n_images_49276.pt\"\n",
    "    sae = SparseAutoencoder.load_from_pretrained(sae_path)\n",
    "    sae.cuda().eval()\n",
    "\n",
    "    # Load model\n",
    "    model = HookedSAEViT.from_pretrained(\"vit_base_patch16_224\")\n",
    "    model.head = torch.nn.Linear(model.cfg.d_model, 6)\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        \"./model/vit_b-ImageNet_class_init-frozen_False-dataset_Hyperkvasir_anatomical.pth\", weights_only=False\n",
    "    )\n",
    "    state_dict = checkpoint['model_state_dict'].copy()\n",
    "\n",
    "    if 'lin_head.weight' in state_dict:\n",
    "        state_dict['head.weight'] = state_dict.pop('lin_head.weight')\n",
    "    if 'lin_head.bias' in state_dict:\n",
    "        state_dict['head.bias'] = state_dict.pop('lin_head.bias')\n",
    "\n",
    "    converted_weights = convert_timm_weights(state_dict, model.cfg)\n",
    "    model.load_state_dict(converted_weights)\n",
    "    model.cuda().eval()\n",
    "\n",
    "    return sae, model\n",
    "\n",
    "sae, model = load_models()\n",
    "# Build new dictionary\n",
    "label_map = {2: 3, 3: 2}\n",
    "\n",
    "def custom_target_transform(target):\n",
    "    return label_map.get(target, target)\n",
    "\n",
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None: sample = self.transform(sample)\n",
    "        # Assuming you have a target_transform defined elsewhere\n",
    "        # if self.target_transform is not None: target = self.target_transform(target)\n",
    "        return sample, target, path\n",
    "\n",
    "train_dataset = ImageFolderWithPaths(\n",
    "    \"./hyper-kvasir_imagefolder/train\", \n",
    "    get_processor_for_precached_224_images()\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "layer_id = 7\n",
    "stealth_dict = build_attribution_aligned_feature_dictionary(\n",
    "    model, sae, dataloader, \n",
    "    n_samples=50000,\n",
    "    attribution_dir=\"./results/train/attributions\",\n",
    "    layer_idx=layer_id,\n",
    "    min_occurrences=1,           # Must appear 3+ times\n",
    "    save_path=f\"./sae_dictionaries/sfaf_stealth_l{layer_id}_alignment_min1_32k64.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f4c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 14:54:38 INFO:root: get_activation_fn received: activation_fn=topk, kwargs={'k': 1024}\n",
      "2025-07-09 14:54:38 WARNING:root: Model 'vit_base_patch16_224' is not in the lists of models passing or failing tests. Unclear status. You may want to check that the HookedViT matches the original model under tests/test_loading_clip.py.\n",
      "2025-07-09 14:54:38 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (17): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_pre not set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 14:54:39 INFO:timm.models._builder: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-07-09 14:54:39 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (18): huggingface.co:443\n",
      "2025-07-09 14:54:39 INFO:timm.models._hub: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-07-09 14:54:39 INFO:root: Filling in 2 missing keys with default initialization\n",
      "2025-07-09 14:54:39 WARNING:root: Missing key for weight matrix: head.W_H\n",
      "2025-07-09 14:54:39 INFO:root: Loaded pretrained model vit_base_patch16_224 into HookedTransformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the weights of a timm model to a Prisma ViT\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Converting the weights of a timm model to a Prisma ViT\n",
      "Loading existing S_f/A_f dictionary from ./sae_dictionaries/sfaf_stealth_l10_alignment_min10_32k512.pt\n",
      "Dictionary loaded successfully!\n",
      "Metadata: {'layer_idx': 10, 'patch_size': 16, 'n_samples_processed': 65, 'min_occurrences': 10, 'attribution_dir': './results/train/attributions'}\n",
      "--------------------------------------------------\n",
      "--- Top 20 Features by Mean PFAC (Alignment) ---\n",
      "      feature_id  mean_pfac_corr  std_pfac_corr  cv_pfac_corr  mean_awa_score  ...  mean_gini_score  mean_dice_score  \\\n",
      "1516       19927        0.113865       0.193980      1.703573        0.550749  ...         0.989824         0.041581   \n",
      "1684       21725        0.113816       0.202648      1.780465        1.010488  ...         0.968784         0.065548   \n",
      "1628       12579        0.113499       0.220535      1.943038        1.105776  ...         0.991201         0.054444   \n",
      "1486        8662        0.112895       0.199596      1.767974        0.245323  ...         0.989427         0.022286   \n",
      "2031       12609        0.110416       0.223012      2.019716        4.151282  ...         0.948381         0.115304   \n",
      "1543         305        0.110121       0.181233      1.645741        1.119808  ...         0.964056         0.099302   \n",
      "1632        2305        0.109612       0.193166      1.762263        0.795987  ...         0.921739         0.123744   \n",
      "2090       20230        0.108934       0.271763      2.494730        2.917290  ...         0.976057         0.104926   \n",
      "1515        7154        0.108552       0.198762      1.831007        0.079809  ...         0.993230         0.004706   \n",
      "2174       16705        0.108303       0.278465      2.571137        1.977661  ...         0.983369         0.093290   \n",
      "1840       22927        0.108293       0.198986      1.837449        3.002810  ...         0.865456         0.163324   \n",
      "2021       19381        0.106907       0.254113      2.376941        1.040330  ...         0.985887         0.074189   \n",
      "1615       16160        0.105350       0.203127      1.928100        0.640812  ...         0.985573         0.077577   \n",
      "2154       15793        0.105334       0.269284      2.556460        2.823756  ...         0.982217         0.109067   \n",
      "1749       23055        0.104654       0.198241      1.894234        0.172964  ...         0.989724         0.015464   \n",
      "1672       13589        0.104611       0.211850      2.025097        1.054687  ...         0.987854         0.071128   \n",
      "1838       13651        0.103246       0.231434      2.241553        1.188565  ...         0.987248         0.074877   \n",
      "1614        6632        0.101735       0.206270      2.027506        1.078934  ...         0.986218         0.091322   \n",
      "1574       23660        0.101195       0.196339      1.940188        0.608106  ...         0.987418         0.075199   \n",
      "1697         132        0.100224       0.210316      2.098431        0.961133  ...         0.986611         0.065128   \n",
      "2027       13673        0.099691       0.205925      2.065610        4.129117  ...         0.834463         0.188336   \n",
      "2200       13337        0.098699       0.261828      2.652763        1.640509  ...         0.968149         0.093444   \n",
      "1983       14617        0.097159       0.242260      2.493412        0.535508  ...         0.980237         0.026786   \n",
      "1763        7736        0.096902       0.221256      2.283265        0.816244  ...         0.988713         0.062865   \n",
      "2121       21686        0.096060       0.205124      2.135351        2.167991  ...         0.902990         0.163082   \n",
      "1437       11908        0.094426       0.147975      1.567086        2.587070  ...         0.796822         0.162220   \n",
      "1415        5839        0.093723       0.160134      1.708570        1.161519  ...         0.972275         0.070700   \n",
      "1336       24103        0.092862       0.148638      1.600615        0.250637  ...         0.991583         0.015143   \n",
      "2217        1887        0.092575       0.250476      2.705617        1.137429  ...         0.932100         0.102355   \n",
      "1702       16999        0.092569       0.206953      2.235642        0.052215  ...         0.989662         0.013732   \n",
      "1986       13696        0.092372       0.218528      2.365717        1.407620  ...         0.981030         0.085045   \n",
      "1801       23390        0.092300       0.189701      2.055254        0.489073  ...         0.976818         0.045234   \n",
      "1691       10723        0.092111       0.178851      1.941661        0.579525  ...         0.981961         0.057045   \n",
      "1806       15756        0.092109       0.208553      2.264168        1.342918  ...         0.975113         0.092665   \n",
      "2158       11100        0.091859       0.250827      2.730543        2.444783  ...         0.944537         0.124741   \n",
      "1818        1530        0.091802       0.220581      2.402774        0.145018  ...         0.992126         0.018006   \n",
      "2140       13765        0.091604       0.252817      2.759875        1.680545  ...         0.985148         0.085808   \n",
      "1737        4465        0.090633       0.200006      2.206734        0.865315  ...         0.984962         0.086372   \n",
      "1964       23269        0.089819       0.206925      2.303780        0.964439  ...         0.984392         0.061378   \n",
      "1484       19142        0.089651       0.175755      1.960404        1.019659  ...         0.980207         0.056752   \n",
      "1811        6453        0.089032       0.213238      2.395042        0.442364  ...         0.991442         0.009848   \n",
      "2066        6702        0.088830       0.250599      2.821072        1.392950  ...         0.986958         0.060888   \n",
      "1273        7915        0.088692       0.107945      1.217061        2.401071  ...         0.809521         0.243457   \n",
      "1621        9557        0.088025       0.190155      2.160221        0.694362  ...         0.992943         0.038240   \n",
      "1366        1546        0.088011       0.150403      1.708884        0.907136  ...         0.968858         0.069820   \n",
      "2032       17331        0.086616       0.243078      2.806368        1.871905  ...         0.982937         0.090576   \n",
      "1857        3226        0.086352       0.226983      2.628541        0.413754  ...         0.983421         0.053235   \n",
      "1492        3785        0.086341       0.165771      1.919933        1.300351  ...         0.972857         0.087295   \n",
      "2082       11520        0.086217       0.248106      2.877675        0.805882  ...         0.989822         0.052609   \n",
      "1794       13129        0.085829       0.203763      2.374024        0.754461  ...         0.989091         0.061139   \n",
      "\n",
      "      consistency_score  occurrences  classes_activated  \n",
      "1516          -0.216949           14                [2]  \n",
      "1684          -0.270444           20       [2, 3, 4, 5]  \n",
      "1628          -0.256657           10                [2]  \n",
      "1486          -0.207898           10                [2]  \n",
      "2031          -0.412493           38    [0, 2, 3, 4, 5]  \n",
      "1543          -0.225991           23             [2, 5]  \n",
      "1632          -0.258265           21          [2, 4, 5]  \n",
      "2090          -0.440944           14                [2]  \n",
      "1515          -0.216309           10             [2, 5]  \n",
      "2174          -0.491824           17                [2]  \n",
      "1840          -0.329894           37    [0, 2, 3, 4, 5]  \n",
      "2021          -0.408137           15                [2]  \n",
      "1615          -0.250789           12             [2, 5]  \n",
      "2154          -0.473871           17                [2]  \n",
      "1749          -0.293435           22    [0, 2, 3, 4, 5]  \n",
      "1672          -0.266473           11                [2]  \n",
      "1838          -0.328790           12                [2]  \n",
      "1614          -0.250659           10                [2]  \n",
      "1574          -0.236419           11                [2]  \n",
      "1697          -0.273562           11                [2]  \n",
      "2027          -0.411245           47    [0, 2, 3, 4, 5]  \n",
      "2200          -0.511482           22          [2, 4, 5]  \n",
      "1983          -0.392935           14             [0, 4]  \n",
      "1763          -0.298182           10                [2]  \n",
      "2121          -0.456932           65    [0, 2, 3, 4, 5]  \n",
      "1437          -0.190380           34          [2, 4, 5]  \n",
      "1415          -0.179840           14          [2, 4, 5]  \n",
      "1336          -0.133741           10          [2, 4, 5]  \n",
      "2217          -0.547232           31       [0, 2, 4, 5]  \n",
      "1702          -0.274276           10             [2, 4]  \n",
      "1986          -0.395554           22             [2, 5]  \n",
      "1801          -0.313517           24       [2, 3, 4, 5]  \n",
      "1691          -0.271965           22       [0, 2, 3, 4]  \n",
      "1806          -0.315329           14             [2, 4]  \n",
      "2158          -0.476218           19                [2]  \n",
      "1818          -0.319999           11             [2, 5]  \n",
      "2140          -0.465959           17                [2]  \n",
      "1737          -0.288635           13                [2]  \n",
      "1964          -0.385955           26          [2, 4, 5]  \n",
      "1484          -0.206462           10             [2, 5]  \n",
      "1811          -0.318576           12          [2, 4, 5]  \n",
      "2066          -0.426910           13                [2]  \n",
      "1273          -0.050806           13          [2, 4, 5]  \n",
      "1621          -0.253778           11             [2, 5]  \n",
      "1366          -0.149604           10             [2, 5]  \n",
      "2032          -0.412907           13                [2]  \n",
      "1857          -0.337211           10                [2]  \n",
      "1492          -0.209615           13             [2, 5]  \n",
      "2082          -0.438398           14                [2]  \n",
      "1794          -0.311228           13                [2]  \n",
      "\n",
      "[50 rows x 11 columns]\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transmm_sfaf import load_models, get_processor_for_precached_224_images, IDX2CLS\n",
    "from transmm_sfaf import load_or_build_sf_af_dictionary\n",
    "\n",
    "sae, model = load_models()\n",
    "layer_idx = 10\n",
    "stealth_dict = load_or_build_sf_af_dictionary(\n",
    "    model,\n",
    "    sae,\n",
    "    n_samples=50000,\n",
    "    layer_idx=layer_idx,\n",
    "    dict_path=f\"./sae_dictionaries/sfaf_stealth_l{layer_idx}_alignment_min10_32k512.pt\",\n",
    "    rebuild=False\n",
    ")\n",
    "\n",
    "print(\"Dictionary loaded successfully!\")\n",
    "print(\"Metadata:\", stealth_dict['metadata'])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Convert to a Pandas DataFrame for easy analysis ---\n",
    "feature_stats = stealth_dict.get('feature_stats', {})\n",
    "if not feature_stats:\n",
    "    print(\"No feature statistics found in the dictionary.\")\n",
    "else:\n",
    "    # We convert the dictionary of features into a list of dictionaries, then to a DataFrame\n",
    "    feature_list = []\n",
    "    for feat_id, stats in feature_stats.items():\n",
    "        # Create a flat dictionary for the DataFrame row\n",
    "        row = {'feature_id': feat_id}\n",
    "        # Add all stats except the bulky raw_metrics\n",
    "        row.update({k: v for k, v in stats.items() if k != 'raw_metrics'})\n",
    "        feature_list.append(row)\n",
    "\n",
    "    df = pd.DataFrame(feature_list)\n",
    "\n",
    "    # Set display options for better readability\n",
    "    pd.set_option('display.max_rows', 50)\n",
    "    pd.set_option('display.width', 120)\n",
    "    \n",
    "    print(\"--- Top 20 Features by Mean PFAC (Alignment) ---\")\n",
    "    # PFAC = Patch-Feature Attribution Correlation\n",
    "    print(df.sort_values(by='mean_pfac_corr', ascending=False).head(100).tail(50))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea5c981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "      TOP 20 CORRELATED FEATURES PER CLASS (ranked by avg_pfac_corr) for Layer 10\n",
      "      (Minimum Occurrences per Class: 10)\n",
      "================================================================================\n",
      "PFAC = Patch-Feature Attribution Correlation\n",
      "\n",
      "                           class_name  avg_pfac_corr  occurrences\n",
      "class_id feature_id                                              \n",
      "5        12609                 z-line       0.340186           11\n",
      "         13673                 z-line       0.336493           11\n",
      "         14977                 z-line       0.285648           10\n",
      "         12153                 z-line       0.270660           11\n",
      "         10860                 z-line       0.270145           11\n",
      "         5286                  z-line       0.270063           10\n",
      "         465                   z-line       0.268521           11\n",
      "         8735                  z-line       0.265743           10\n",
      "         2605                  z-line       0.261874           11\n",
      "         14400                 z-line       0.258170           11\n",
      "         2203                  z-line       0.253668           11\n",
      "         10315                 z-line       0.240885           11\n",
      "         15498                 z-line       0.238697           11\n",
      "         22110                 z-line       0.221261           11\n",
      "         5858                  z-line       0.220652           11\n",
      "         17519                 z-line       0.219016           11\n",
      "         2073                  z-line       0.218902           10\n",
      "         16071                 z-line       0.218677           11\n",
      "         12320                 z-line       0.216653           11\n",
      "         11633                 z-line       0.208127           11\n",
      "2        418         retroflex-rectum       0.159776           10\n",
      "         24146       retroflex-rectum       0.158485           12\n",
      "         20365       retroflex-rectum       0.153805           12\n",
      "         6214        retroflex-rectum       0.153664           13\n",
      "         7675        retroflex-rectum       0.152340           11\n",
      "         10067       retroflex-rectum       0.145220           12\n",
      "         7829        retroflex-rectum       0.141076           10\n",
      "         4699        retroflex-rectum       0.138549           12\n",
      "         9909        retroflex-rectum       0.136109           10\n",
      "         2012        retroflex-rectum       0.135559           11\n",
      "0        12311                  cecum       0.135364           11\n",
      "2        17943       retroflex-rectum       0.134835           13\n",
      "         8321        retroflex-rectum       0.132035           13\n",
      "         21686       retroflex-rectum       0.129880           22\n",
      "         12403       retroflex-rectum       0.127418           10\n",
      "         4645        retroflex-rectum       0.126142           14\n",
      "         10405       retroflex-rectum       0.123743           14\n",
      "         2827        retroflex-rectum       0.122954           11\n",
      "         12540       retroflex-rectum       0.120600           11\n",
      "         12892       retroflex-rectum       0.120467           15\n",
      "         536         retroflex-rectum       0.120285           12\n",
      "0        9986                   cecum       0.103188           21\n",
      "         1082                   cecum       0.096051           21\n",
      "         13644                  cecum       0.092963           21\n",
      "         14286                  cecum       0.091915           10\n",
      "         20713                  cecum       0.089828           21\n",
      "         17383                  cecum       0.084935           21\n",
      "         2136                   cecum       0.084453           10\n",
      "         20514                  cecum       0.082800           14\n",
      "         19397                  cecum       0.081987           21\n",
      "         10233                  cecum       0.081252           10\n",
      "         10776                  cecum       0.080104           21\n",
      "         24480                  cecum       0.079687           16\n",
      "         3281                   cecum       0.079005           20\n",
      "         13289                  cecum       0.076888           21\n",
      "         15970                  cecum       0.075885           21\n",
      "         3448                   cecum       0.075645           12\n",
      "         5414                   cecum       0.074755           13\n",
      "         2333                   cecum       0.074449           14\n",
      "         14327                  cecum       0.074136           21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# Assuming these are already loaded from your setup code\n",
    "# from transmm_sfaf import load_models, IDX2CLS, load_or_build_sf_af_dictionary\n",
    "# sae, model = load_models()\n",
    "# layer_idx = 7\n",
    "# stealth_dict = load_or_build_sf_af_dictionary(...)\n",
    "# print(\"Dictionary loaded successfully!\")\n",
    "\n",
    "# --- Configuration ---\n",
    "TOP_K_PER_CLASS = 20\n",
    "# Add a minimum occurrence filter to ensure statistical significance\n",
    "MIN_OCCURRENCES_FOR_CLASS = 10\n",
    "\n",
    "# --- 1. Process Raw Data into a Per-Class Structure ---\n",
    "\n",
    "feature_stats = stealth_dict.get('feature_stats', {})\n",
    "if not feature_stats:\n",
    "    print(\"No feature statistics found in the dictionary.\")\n",
    "else:\n",
    "    per_class_feature_data = []\n",
    "\n",
    "    # Iterate through each feature in the dictionary\n",
    "    for feat_id, stats in feature_stats.items():\n",
    "        raw_metrics = stats.get('raw_metrics', {})\n",
    "        pfac_corrs = raw_metrics.get('pfac_corrs', [])\n",
    "        classes = raw_metrics.get('classes', [])\n",
    "\n",
    "        if not pfac_corrs or not classes:\n",
    "            continue\n",
    "\n",
    "        # Group correlations by class for the current feature\n",
    "        corrs_by_class = {}\n",
    "        for corr, cls_id in zip(pfac_corrs, classes):\n",
    "            if cls_id not in corrs_by_class:\n",
    "                corrs_by_class[cls_id] = []\n",
    "            corrs_by_class[cls_id].append(corr)\n",
    "\n",
    "        # Calculate stats for each class this feature appeared in\n",
    "        for cls_id, corrs_list in corrs_by_class.items():\n",
    "            valid_corrs = [c for c in corrs_list if not np.isnan(c)]\n",
    "            occurrences = len(valid_corrs)\n",
    "\n",
    "            # --- NEW: Filter based on minimum occurrences ---\n",
    "            if occurrences < MIN_OCCURRENCES_FOR_CLASS:\n",
    "                continue  # Skip if this feature-class pair is too rare\n",
    "\n",
    "            avg_corr = np.mean(valid_corrs)\n",
    "            \n",
    "            per_class_feature_data.append({\n",
    "                'feature_id': feat_id,\n",
    "                'class_id': cls_id,\n",
    "                'class_name': IDX2CLS.get(cls_id, 'Unknown'), # Still useful for display\n",
    "                'avg_pfac_corr': avg_corr, # The key metric for ranking\n",
    "                'occurrences': occurrences, # How often it activated for this class\n",
    "            })\n",
    "\n",
    "    # --- 2. Create the DataFrame and Perform Analysis ---\n",
    "    \n",
    "    if not per_class_feature_data:\n",
    "        print(f\"No valid per-class feature data could be extracted with min occurrences >= {MIN_OCCURRENCES_FOR_CLASS}.\")\n",
    "    else:\n",
    "        df_per_class = pd.DataFrame(per_class_feature_data)\n",
    "\n",
    "        # Set display options for better readability\n",
    "        pd.set_option('display.max_rows', 500)\n",
    "        pd.set_option('display.max_columns', 10)\n",
    "        pd.set_option('display.width', 150)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"      TOP {TOP_K_PER_CLASS} CORRELATED FEATURES PER CLASS (ranked by avg_pfac_corr) for Layer {layer_idx}\")\n",
    "        print(f\"      (Minimum Occurrences per Class: {MIN_OCCURRENCES_FOR_CLASS})\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"PFAC = Patch-Feature Attribution Correlation\\n\")\n",
    "\n",
    "        # --- 3. Group by Class ID and Display Results ---\n",
    "        \n",
    "        # Sort by correlation, then group by class ID and take the top N\n",
    "        top_features_per_class = (\n",
    "            df_per_class\n",
    "            .sort_values(by='avg_pfac_corr', ascending=False)\n",
    "            .groupby('class_id', sort=False) # Group by class_id instead of class_name\n",
    "            .head(TOP_K_PER_CLASS)\n",
    "        )\n",
    "        \n",
    "        # Set the multi-index using class_id and feature_id for clear grouping\n",
    "        top_features_per_class = top_features_per_class.set_index(['class_id', 'feature_id'])\n",
    "\n",
    "        print(top_features_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98d2de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- Statistics for Specific Features: [8, 9, 12, 23, 26] ---\n",
      "feature_id                      12                  23        9             8                   26\n",
      "mean_pfac_corr            -0.01718           -0.018236  0.073728      0.006976            0.005308\n",
      "std_pfac_corr             0.085807            0.071139  0.162014      0.086522            0.094934\n",
      "cv_pfac_corr              4.994277            3.900749  2.197426     12.400353           17.882137\n",
      "mean_awa_score           -1.019369           -0.569488  1.565944      0.211732            0.099672\n",
      "std_awa_score             3.391973            2.094591  3.699006      2.186467            1.626569\n",
      "mean_gini_score           0.967501             0.97359  0.990795      0.985377            0.991927\n",
      "mean_dice_score           0.043068            0.040191  0.046066      0.034196            0.016611\n",
      "consistency_score         0.561156            0.429977 -0.498712     -0.534286           -0.647333\n",
      "occurrences                   3559                3388       283           826                1371\n",
      "classes_activated  [1, 2, 3, 4, 5]  [0, 1, 2, 3, 4, 5]    [2, 5]  [2, 3, 4, 5]  [0, 1, 2, 3, 4, 5]\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the list of feature IDs you want to inspect\n",
    "specific_feature_ids = [8, 9, 12, 23, 26]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"--- Statistics for Specific Features: {specific_feature_ids} ---\")\n",
    "\n",
    "# 2. Filter the DataFrame to get only the rows for these features\n",
    "# .isin() checks if the value in the 'feature_id' column is present in your list\n",
    "specific_features_df = df[df['feature_id'].isin(specific_feature_ids)]\n",
    "\n",
    "# 3. Print the resulting filtered DataFrame\n",
    "if not specific_features_df.empty:\n",
    "    # We can transpose (.T) the DataFrame for better readability when there are few features\n",
    "    # This turns columns into rows, which is often easier to read.\n",
    "    print(specific_features_df.set_index('feature_id').T)\n",
    "else:\n",
    "    print(\"None of the specified features were found in the dictionary.\")\n",
    "    print(\"This might be because they didn't meet the min_occurrences threshold.\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31ebfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 14:47:40 INFO:root: get_activation_fn received: activation_fn=topk, kwargs={'k': 128}\n",
      "2025-07-07 14:47:40 WARNING:root: Model 'vit_base_patch16_224' is not in the lists of models passing or failing tests. Unclear status. You may want to check that the HookedViT matches the original model under tests/test_loading_clip.py.\n",
      "2025-07-07 14:47:41 DEBUG:urllib3.connectionpool: Resetting dropped connection: huggingface.co\n",
      "2025-07-07 14:47:41 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "2025-07-07 14:47:41 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /api/resolve-cache/models/timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/063c6c38a5d8510b2e57df480445e94b231dad2c/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_pre not set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 14:47:42 INFO:timm.models._builder: Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n",
      "2025-07-07 14:47:42 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /timm/vit_base_patch16_224.augreg2_in21k_ft_in1k/resolve/main/model.safetensors HTTP/1.1\" 302 0\n",
      "2025-07-07 14:47:42 INFO:timm.models._hub: [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-07-07 14:47:42 INFO:root: Filling in 2 missing keys with default initialization\n",
      "2025-07-07 14:47:42 WARNING:root: Missing key for weight matrix: head.W_H\n",
      "2025-07-07 14:47:42 INFO:root: Loaded pretrained model vit_base_patch16_224 into HookedTransformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the weights of a timm model to a Prisma ViT\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Converting the weights of a timm model to a Prisma ViT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building comprehensive dict: 100%|██████████| 750/750 [01:39<00:00,  7.57it/s, Processed=750, Features Found=936]\n",
      "2025-07-07 14:49:22 INFO:root: Comprehensive dictionary saved to ./sae_dictionaries/sfaf_stealth_l9_alignment_comp.pt\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from transmm_sfaf import load_models, get_processor_for_precached_224_images, IDX2CLS\n",
    "import torch\n",
    "\n",
    "\n",
    "sae, model = load_models()\n",
    "# Build new dictionary\n",
    "label_map = {2: 3, 3: 2}\n",
    "\n",
    "def custom_target_transform(target):\n",
    "    return label_map.get(target, target)\n",
    "\n",
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None: sample = self.transform(sample)\n",
    "        # Assuming you have a target_transform defined elsewhere\n",
    "        # if self.target_transform is not None: target = self.target_transform(target)\n",
    "        return sample, target, path\n",
    "\n",
    "train_dataset = ImageFolderWithPaths(\n",
    "    \"./hyper-kvasir_imagefolder/dev\", \n",
    "    get_processor_for_precached_224_images()\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "layer_id = 9\n",
    "stealth_dict = build_comprehensive_feature_dictionary(\n",
    "    model, sae, dataloader, \n",
    "    n_samples=50000,\n",
    "    attribution_dir=\"./results/dev/attributions\",\n",
    "    layer_idx=layer_id,\n",
    "    min_occurrences=3,           # Must appear 3+ times\n",
    "    save_path=f\"./sae_dictionaries/sfaf_stealth_l{layer_id}_alignment_comp.pt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradcamfaith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
